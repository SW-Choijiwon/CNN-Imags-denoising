{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "N4_Multimedia_Termproject.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BQmYo2rqmK4"
      },
      "source": [
        "# Get Dataset from Google Drive  \n",
        "Please upload your dataset on google drive first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vxHB3wfW911",
        "outputId": "07cfe8cd-d5e4-45ae-cfa2-f37eab1f0fb9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbDhYfKKa41s"
      },
      "source": [
        "import os\n",
        "import zipfile\n",
        "import tqdm\n",
        "\n",
        "file_name = \"Multimedia_dataset.zip\"\n",
        "zip_path = os.path.join('/content/drive/MyDrive/Multimedia/Multimedia_dataset/Multimedia_dataset.zip')\n",
        "\n",
        "!cp \"{zip_path}\" .\n",
        "!unzip -q \"{file_name}\"\n",
        "!rm \"{file_name}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmbp65jgq67j"
      },
      "source": [
        "# Noise Transform  \n",
        "If you want to change how much noise you are giving, change the stddev and mean values at 'gaussian_noise' function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyeUBdtKYQSu"
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torchvision import transforms\n",
        "\n",
        "import random\n",
        "\n",
        "class NoiseTransform(object):\n",
        "  def __init__(self, size=180, mode=\"training\"):\n",
        "    super(NoiseTransform, self).__init__()\n",
        "    self.size = size\n",
        "    self.mode = mode\n",
        "  \n",
        "  def gaussian_noise(self, img):\n",
        "    mean = 0\n",
        "    stddev = 25\n",
        "    noise = Variable(torch.zeros(img.size()))\n",
        "    noise = noise.data.normal_(mean, stddev/255.)\n",
        "\n",
        "    return noise\n",
        "\n",
        "  def __call__(self, img):\n",
        "    if (self.mode == \"training\") | (self.mode == \"validation\"):\n",
        "      self.gt_transform = transforms.Compose([\n",
        "        transforms.Resize((self.size, self.size), interpolation=2),\n",
        "        transforms.ToTensor()])\n",
        "      self.noise_transform = transforms.Compose([\n",
        "        transforms.Resize((self.size, self.size), interpolation=2),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Lambda(self.gaussian_noise),\n",
        "      ])\n",
        "      return self.gt_transform(img), self.noise_transform(img)\n",
        "\n",
        "    elif self.mode == \"testing\":\n",
        "      self.gt_transform = transforms.Compose([\n",
        "        transforms.ToTensor()])\n",
        "      return self.gt_transform(img)\n",
        "    else:\n",
        "      return NotImplementedError\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGKbE7uFrWwb"
      },
      "source": [
        "# Dataloader for Noise Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxZsXXZ9YpYp"
      },
      "source": [
        "import torch\n",
        "import torch.utils.data  as data\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class NoiseDataset(data.Dataset):\n",
        "  def __init__(self, root_path, size):\n",
        "    super(NoiseDataset, self).__init__()\n",
        "\n",
        "    self.root_path = root_path\n",
        "    self.size = size\n",
        "    self.transforms = None\n",
        "    self.examples = None\n",
        "\n",
        "  def set_mode(self, mode):\n",
        "    self.mode = mode\n",
        "    self.transforms = NoiseTransform(self.size, mode)\n",
        "    if mode == \"training\":\n",
        "      train_dir = os.path.join(self.root_path, \"train\")\n",
        "      self.examples = [os.path.join(self.root_path, \"train\", dirs) for dirs in os.listdir(train_dir)]\n",
        "    elif mode == \"validation\":\n",
        "      val_dir = os.path.join(self.root_path, \"validation\")\n",
        "      self.examples = [os.path.join(self.root_path, \"validation\", dirs) for dirs in os.listdir(val_dir)]\n",
        "    elif mode == \"testing\":\n",
        "      test_dir = os.path.join(self.root_path, \"test\")\n",
        "      self.examples = [os.path.join(self.root_path, \"test\", dirs) for dirs in os.listdir(test_dir)]\n",
        "    else:\n",
        "      raise NotImplementedError\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.examples)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    file_name = self.examples[idx]\n",
        "    image = Image.open(file_name)\n",
        "\n",
        "\n",
        "    if self.mode == \"testing\":\n",
        "      input_img = self.transforms(image)\n",
        "      sample = {\"img\": input_img}\n",
        "    else:\n",
        "      clean, noise = self.transforms(image)\n",
        "      sample = {\"img\": clean, \"noise\": noise}\n",
        "\n",
        "    return sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDx4vFU-rf5z"
      },
      "source": [
        "# Example for Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53tDtBFLenTz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "675944b1-9a92-4363-f9c9-dfb2de01201c"
      },
      "source": [
        "import torch\n",
        "import torch.utils.data  as data\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms\n",
        "import tqdm\n",
        "from PIL import Image\n",
        "\n",
        "def image_show(img):\n",
        "  if isinstance(img, torch.Tensor):\n",
        "    # PIL image로 바꿔준다.\n",
        "    img = transforms.ToPILImage()(img)\n",
        "  plt.imshow(img)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# Change to your data root directory\n",
        "root_path = \"/content/\"\n",
        "# Depend on runtime setting\n",
        "use_cuda = True\n",
        "\n",
        "train_dataset = NoiseDataset(root_path, 128) #128은 size\n",
        "train_dataset.set_mode(\"training\")\n",
        "\n",
        "\n",
        "# batch=4 단위로 data를 load\n",
        "train_dataloader = data.DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "\"\"\"\n",
        "# tqdm은 진행표시바\n",
        "for i, data in enumerate(tqdm.tqdm(train_dataloader)):\n",
        "  #CUDA는 NVIDIA에서 개발한 GPU 개발 툴로 많은 양의 연산을 동시에 처리하는 것이 목표\n",
        "  if use_cuda:\n",
        "    img = data[\"img\"].to('cuda')\n",
        "    noise = data[\"noise\"].to('cuda')\n",
        "  \n",
        "  model_input = img + noise\n",
        "\n",
        "  # clamp는 최대 최소 값을 정해주는 함수\n",
        "  # (최소~최대 범위에 포함되지 않는게 있으면 그 값을 최소 최대값으로 변경)\n",
        "  noise_image = torch.clamp(model_input, 0, 1)\n",
        "\n",
        "  image_show(img[0])\n",
        "  image_show(noise[0])\n",
        "\n",
        "\n",
        "\n",
        "  input()\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n# tqdm은 진행표시바\\nfor i, data in enumerate(tqdm.tqdm(train_dataloader)):\\n  #CUDA는 NVIDIA에서 개발한 GPU 개발 툴로 많은 양의 연산을 동시에 처리하는 것이 목표\\n  if use_cuda:\\n    img = data[\"img\"].to(\\'cuda\\')\\n    noise = data[\"noise\"].to(\\'cuda\\')\\n  \\n  model_input = img + noise\\n\\n  # clamp는 최대 최소 값을 정해주는 함수\\n  # (최소~최대 범위에 포함되지 않는게 있으면 그 값을 최소 최대값으로 변경)\\n  noise_image = torch.clamp(model_input, 0, 1)\\n\\n  image_show(img[0])\\n  image_show(noise[0])\\n\\n\\n\\n  input()\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsbiLHm6CJ9z"
      },
      "source": [
        "# Loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2b7gIV_LCO_6"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MS_SSIM_L1_LOSS(nn.Module):\n",
        "    # Have to use cuda, otherwise the speed is too slow.\n",
        "    def __init__(self, gaussian_sigmas=[0.5, 1.0, 2.0, 4.0, 8.0],\n",
        "                 data_range = 1.0,\n",
        "                 K=(0.01, 0.03),\n",
        "                 alpha=0.025,\n",
        "                 compensation=200.0,\n",
        "                 cuda_dev=0,):\n",
        "        super(MS_SSIM_L1_LOSS, self).__init__()\n",
        "        self.DR = data_range\n",
        "        self.C1 = (K[0] * data_range) ** 2\n",
        "        self.C2 = (K[1] * data_range) ** 2\n",
        "        self.pad = int(2 * gaussian_sigmas[-1])\n",
        "        self.alpha = alpha\n",
        "        self.compensation=compensation\n",
        "        filter_size = int(4 * gaussian_sigmas[-1] + 1)\n",
        "        g_masks = torch.zeros((3*len(gaussian_sigmas), 1, filter_size, filter_size))\n",
        "        for idx, sigma in enumerate(gaussian_sigmas):\n",
        "            # r0,g0,b0,r1,g1,b1,...,rM,gM,bM\n",
        "            g_masks[3*idx+0, 0, :, :] = self._fspecial_gauss_2d(filter_size, sigma)\n",
        "            g_masks[3*idx+1, 0, :, :] = self._fspecial_gauss_2d(filter_size, sigma)\n",
        "            g_masks[3*idx+2, 0, :, :] = self._fspecial_gauss_2d(filter_size, sigma)\n",
        "        self.g_masks = g_masks.cuda(cuda_dev)\n",
        "\n",
        "    def _fspecial_gauss_1d(self, size, sigma):\n",
        "        \"\"\"Create 1-D gauss kernel\n",
        "        Args:\n",
        "            size (int): the size of gauss kernel\n",
        "            sigma (float): sigma of normal distribution\n",
        "        Returns:\n",
        "            torch.Tensor: 1D kernel (size)\n",
        "        \"\"\"\n",
        "        coords = torch.arange(size).to(dtype=torch.float)\n",
        "        coords -= size // 2\n",
        "        g = torch.exp(-(coords ** 2) / (2 * sigma ** 2))\n",
        "        g /= g.sum()\n",
        "        return g.reshape(-1)\n",
        "\n",
        "    def _fspecial_gauss_2d(self, size, sigma):\n",
        "        \"\"\"Create 2-D gauss kernel\n",
        "        Args:\n",
        "            size (int): the size of gauss kernel\n",
        "            sigma (float): sigma of normal distribution\n",
        "        Returns:\n",
        "            torch.Tensor: 2D kernel (size x size)\n",
        "        \"\"\"\n",
        "        gaussian_vec = self._fspecial_gauss_1d(size, sigma)\n",
        "        return torch.outer(gaussian_vec, gaussian_vec)\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        b, c, h, w = x.shape\n",
        "        mux = F.conv2d(x, self.g_masks, groups=3, padding=self.pad)\n",
        "        muy = F.conv2d(y, self.g_masks, groups=3, padding=self.pad)\n",
        "\n",
        "        mux2 = mux * mux\n",
        "        muy2 = muy * muy\n",
        "        muxy = mux * muy\n",
        "\n",
        "        sigmax2 = F.conv2d(x * x, self.g_masks, groups=3, padding=self.pad) - mux2\n",
        "        sigmay2 = F.conv2d(y * y, self.g_masks, groups=3, padding=self.pad) - muy2\n",
        "        sigmaxy = F.conv2d(x * y, self.g_masks, groups=3, padding=self.pad) - muxy\n",
        "\n",
        "        # l(j), cs(j) in MS-SSIM\n",
        "        l  = (2 * muxy    + self.C1) / (mux2    + muy2    + self.C1)  # [B, 15, H, W]\n",
        "        cs = (2 * sigmaxy + self.C2) / (sigmax2 + sigmay2 + self.C2)\n",
        "\n",
        "        lM = l[:, -1, :, :] * l[:, -2, :, :] * l[:, -3, :, :]\n",
        "        PIcs = cs.prod(dim=1)\n",
        "\n",
        "        loss_ms_ssim = 1 - lM*PIcs  # [B, H, W]\n",
        "\n",
        "        loss_l1 = F.l1_loss(x, y, reduction='none')  # [B, 3, H, W]\n",
        "        # average l1 loss in 3 channels\n",
        "        gaussian_l1 = F.conv2d(loss_l1, self.g_masks.narrow(dim=0, start=-3, length=3),\n",
        "                               groups=3, padding=self.pad).mean(1)  # [B, H, W]\n",
        "\n",
        "        loss_mix = self.alpha * loss_ms_ssim + (1 - self.alpha) * gaussian_l1 / self.DR\n",
        "        loss_mix = self.compensation*loss_mix\n",
        "\n",
        "        return loss_mix.mean()\n",
        "\n",
        "# https://github.com/psyrocloud/MS-SSIM_L1_LOSS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtESD4NV4Wks"
      },
      "source": [
        "# UNet Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVJI37g45PLz"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "## 네트워크 구축하기\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, nch, nker=64, learning_type=\"plain\", norm=\"bnorm\"):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        self.learning_type = learning_type\n",
        "\n",
        "        # Contracting path\n",
        "        self.enc1_1 = CBR2d(in_channels=nch, out_channels=1 * nker, norm=norm)\n",
        "        self.enc1_2 = CBR2d(in_channels=1 * nker, out_channels=1 * nker, norm=norm)\n",
        "\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.enc2_1 = CBR2d(in_channels=nker, out_channels=2 * nker, norm=norm)\n",
        "        self.enc2_2 = CBR2d(in_channels=2 * nker, out_channels=2 * nker, norm=norm)\n",
        "\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.enc3_1 = CBR2d(in_channels=2 * nker, out_channels=4 * nker, norm=norm)\n",
        "        self.enc3_2 = CBR2d(in_channels=4 * nker, out_channels=4 * nker, norm=norm)\n",
        "\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.enc4_1 = CBR2d(in_channels=4 * nker, out_channels=8 * nker, norm=norm)\n",
        "        self.enc4_2 = CBR2d(in_channels=8 * nker, out_channels=8 * nker, norm=norm)\n",
        "\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.enc5_1 = CBR2d(in_channels=8 * nker, out_channels=16 * nker, norm=norm)\n",
        "\n",
        "\n",
        "        # Expansive path\n",
        "        self.dec5_1 = CBR2d(in_channels=16 * nker, out_channels=8 * nker, norm=norm)\n",
        "\n",
        "        self.unpool4 = nn.ConvTranspose2d(in_channels=8 * nker, out_channels=8 * nker,\n",
        "                                          kernel_size=2, stride=2, padding=0, bias=True)\n",
        "\n",
        "        self.dec4_2 = CBR2d(in_channels=2 * 8 * nker, out_channels=8 * nker, norm=norm)\n",
        "        self.dec4_1 = CBR2d(in_channels=8 * nker, out_channels=4 * nker, norm=norm)\n",
        "\n",
        "        self.unpool3 = nn.ConvTranspose2d(in_channels=4 * nker, out_channels=4 * nker,\n",
        "                                          kernel_size=2, stride=2, padding=0, bias=True)\n",
        "\n",
        "        self.dec3_2 = CBR2d(in_channels=2 * 4 * nker, out_channels=4 * nker, norm=norm)\n",
        "        self.dec3_1 = CBR2d(in_channels=4 * nker, out_channels=2 * nker, norm=norm)\n",
        "\n",
        "        self.unpool2 = nn.ConvTranspose2d(in_channels=2 * nker, out_channels=2 * nker,\n",
        "                                          kernel_size=2, stride=2, padding=0, bias=True)\n",
        "\n",
        "        self.dec2_2 = CBR2d(in_channels=2 * 2 * nker, out_channels=2 * nker, norm=norm)\n",
        "        self.dec2_1 = CBR2d(in_channels=2 * nker, out_channels=1 * nker, norm=norm)\n",
        "\n",
        "        self.unpool1 = nn.ConvTranspose2d(in_channels=1 * nker, out_channels=1 * nker,\n",
        "                                          kernel_size=2, stride=2, padding=0, bias=True)\n",
        "\n",
        "        self.dec1_2 = CBR2d(in_channels=2 * 1 * nker, out_channels=1 * nker, norm=norm)\n",
        "        self.dec1_1 = CBR2d(in_channels=1 * nker, out_channels=1 * nker, norm=norm)\n",
        "\n",
        "        self.fc = nn.Conv2d(in_channels=1 * nker, out_channels=nch, kernel_size=1, stride=1, padding=0, bias=True)\n",
        "\n",
        "    # Unet Layer 연결 (Forwarding)\n",
        "    # 위에서 정의한 것을 순서대로 실행한다고 생각하면 됨\n",
        "    def forward(self, x):\n",
        "        # forward encoder\n",
        "        enc1_1 = self.enc1_1(x)\n",
        "        enc1_2 = self.enc1_2(enc1_1)\n",
        "        pool1 = self.pool1(enc1_2)\n",
        "\n",
        "        enc2_1 = self.enc2_1(pool1)\n",
        "        enc2_2 = self.enc2_2(enc2_1)\n",
        "        pool2 = self.pool2(enc2_2)\n",
        "\n",
        "        enc3_1 = self.enc3_1(pool2)\n",
        "        enc3_2 = self.enc3_2(enc3_1)\n",
        "        pool3 = self.pool3(enc3_2)\n",
        "\n",
        "        enc4_1 = self.enc4_1(pool3)\n",
        "        enc4_2 = self.enc4_2(enc4_1)\n",
        "        pool4 = self.pool4(enc4_2)\n",
        "\n",
        "        enc5_1 = self.enc5_1(pool4)\n",
        "\n",
        "        # forward decoder\n",
        "        dec5_1 = self.dec5_1(enc5_1)\n",
        "\n",
        "        unpool4 = self.unpool4(dec5_1)\n",
        "        # cat은 이전 step의 output channel과 skip connection을 연결해주는 부분\n",
        "        # dim = [0: batch, 1: channel, 2: height, 3: width] <- dim은 해당 방향을 알려준다.\n",
        "        cat4 = torch.cat((unpool4, enc4_2), dim=1)\n",
        "        dec4_2 = self.dec4_2(cat4)\n",
        "        dec4_1 = self.dec4_1(dec4_2)\n",
        "\n",
        "        unpool3 = self.unpool3(dec4_1)\n",
        "        cat3 = torch.cat((unpool3, enc3_2), dim=1)\n",
        "        dec3_2 = self.dec3_2(cat3)\n",
        "        dec3_1 = self.dec3_1(dec3_2)\n",
        "\n",
        "        unpool2 = self.unpool2(dec3_1)\n",
        "        cat2 = torch.cat((unpool2, enc2_2), dim=1)\n",
        "        dec2_2 = self.dec2_2(cat2)\n",
        "        dec2_1 = self.dec2_1(dec2_2)\n",
        "\n",
        "        unpool1 = self.unpool1(dec2_1)\n",
        "        cat1 = torch.cat((unpool1, enc1_2), dim=1)\n",
        "        dec1_2 = self.dec1_2(cat1)\n",
        "        dec1_1 = self.dec1_1(dec1_2)\n",
        "\n",
        "        if self.learning_type == \"plain\":\n",
        "            x = self.fc(dec1_1)\n",
        "        elif self.learning_type == \"residual\":\n",
        "            x = x + self.fc(dec1_1)\n",
        "\n",
        "        return x\n",
        "  \n",
        "## 네트워크 저장하기\n",
        "def save(ckpt_dir, net, optim, epoch):\n",
        "    if not os.path.exists(ckpt_dir):\n",
        "        os.makedirs(ckpt_dir)\n",
        "\n",
        "    torch.save({'net': net.state_dict(), 'optim': optim.state_dict()},\n",
        "               \"%s/model_epoch%d.pth\" % (ckpt_dir, epoch))\n",
        "\n",
        "## 네트워크 불러오기\n",
        "def load(ckpt_dir, net, optim):\n",
        "    if not os.path.exists(ckpt_dir):\n",
        "        epoch = 0\n",
        "        return net, optim, epoch\n",
        "\n",
        "    ckpt_lst = os.listdir(ckpt_dir)\n",
        "    ckpt_lst.sort(key=lambda f: int(''.join(filter(str.isdigit, f))))\n",
        "\n",
        "    dict_model = torch.load('%s/%s' % (ckpt_dir, ckpt_lst[-1]))\n",
        "\n",
        "    net.load_state_dict(dict_model['net'])\n",
        "    optim.load_state_dict(dict_model['optim'])\n",
        "    epoch = int(ckpt_lst[-1].split('epoch')[1].split('.pth')[0])\n",
        "\n",
        "    return net, optim, epoch\n",
        "\n",
        "# convolution, batch normalizatoin, Relu\n",
        "class CBR2d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True, norm=\"bnorm\", relu=0.0):\n",
        "        super().__init__()\n",
        "\n",
        "        layers = []\n",
        "        layers += [nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
        "                             kernel_size=kernel_size, stride=stride, padding=padding,\n",
        "                             bias=bias)]\n",
        "\n",
        "        if not norm is None:\n",
        "            if norm == \"bnorm\":\n",
        "                layers += [nn.BatchNorm2d(num_features=out_channels)]\n",
        "            elif norm == \"inorm\":\n",
        "                layers += [nn.InstanceNorm2d(num_features=out_channels)]\n",
        "\n",
        "        if not relu is None and relu >= 0.0:\n",
        "            layers += [nn.ReLU() if relu == 0 else nn.LeakyReLU(relu)]\n",
        "\n",
        "        self.cbr = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.cbr(x)\n",
        "\n",
        "\n",
        "def psnr (mse):\n",
        "  if mse == 100:\n",
        "    return 100\n",
        "  else:\n",
        "    return 20*math.log(255/math.sqrt(mse))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xh1CUC654Q87"
      },
      "source": [
        "# Trainer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcjIWFPo4VeW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e28e34c4-23fe-4395-9cd4-eec468d2ad07"
      },
      "source": [
        "import argparse\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "#import pytorch_ssim\n",
        "!pip install pytorch_msssim \n",
        "import pytorch_msssim\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "## 트레이닝 파라메터 설정하기\n",
        "train_continue = \"off\"\n",
        "\n",
        "lr = 1e-3 #learning rate\n",
        "batch_size = 50\n",
        "num_epoch = 100\n",
        "\n",
        "ckpt_dir = \"/content/drive/MyDrive/Multimedia/Test1/checkpoint/\"\n",
        "log_dir = \"/content/drive/MyDrive/Multimedia/Test1/log\"\n",
        "result_dir = \"/content/drive/MyDrive/Multimedia/Test1/result\"\n",
        "\n",
        "task = 'denoising'\n",
        "opts = ['random', 4]\n",
        "\n",
        "\n",
        "nch = 3\n",
        "nker = 64\n",
        "\n",
        "learning_type = 'plain'\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "\n",
        "print(\"learning rate: %.4e\" % lr)\n",
        "print(\"batch size: %d\" % batch_size)\n",
        "print(\"number of epoch: %d\" % num_epoch)\n",
        "\n",
        "print(\"task: %s\" % task)\n",
        "print(\"opts: %s\" % opts)\n",
        "\n",
        "print(\"learning type: %s\" % learning_type)\n",
        "\n",
        "print(\"ckpt dir: %s\" % ckpt_dir)\n",
        "print(\"log dir: %s\" % log_dir)\n",
        "print(\"result dir: %s\" % result_dir)\n",
        "\n",
        "print(\"device: %s\" % device)\n",
        "\n",
        "## 디렉토리 생성하기\n",
        "result_dir_train = os.path.join(result_dir, 'train')\n",
        "result_dir_val = os.path.join(result_dir, 'val')\n",
        "\n",
        "if not os.path.exists(result_dir):\n",
        "    os.makedirs(os.path.join(result_dir_train, 'png'))\n",
        "    # os.makedirs(os.path.join(result_dir_train, 'numpy'))\n",
        "\n",
        "    os.makedirs(os.path.join(result_dir_val, 'png'))\n",
        "    # os.makedirs(os.path.join(result_dir_val, 'numpy'))\n",
        "\n",
        "    os.makedirs(os.path.join(result_dir_test, 'png'))\n",
        "    os.makedirs(os.path.join(result_dir_test, 'numpy'))\n",
        "\n",
        "\n",
        "# Change to your data root directory\n",
        "root_path = \"/content/\"\n",
        "# Depend on runtime setting\n",
        "use_cuda = True\n",
        "\n",
        "\n",
        "\n",
        "# training dataset 불러오기\n",
        "dataset_train = NoiseDataset(root_path, 128) #128은 size\n",
        "dataset_train.set_mode(\"training\")\n",
        "loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "\n",
        "# validation dataset불러오기\n",
        "dataset_val = NoiseDataset(root_path, 128) #128은 size\n",
        "dataset_val.set_mode(\"validation\")\n",
        "loader_val = DataLoader(dataset_val, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "\n",
        "# 그밖에 부수적인 variables 설정하기\n",
        "num_data_train = len(dataset_train)\n",
        "num_data_val = len(dataset_val)\n",
        "\n",
        "num_batch_train = np.ceil(num_data_train / batch_size)\n",
        "num_batch_val = np.ceil(num_data_val / batch_size)\n",
        "\n",
        "\n",
        "\n",
        "## 네트워크 생성하기\n",
        "net = UNet(nch=nch, nker=nker, learning_type=learning_type).to(device)\n",
        "\n",
        "\n",
        "## 손실함수 정의하기\n",
        "#fn_loss = nn.MSELoss().to(device)\n",
        "#fn_loss = nn.L1Loss().to(device)\n",
        "#fn_loss = pytorch_msssim.SSIM().to(device)\n",
        "fn_loss = MS_SSIM_L1_LOSS()\n",
        "mse_loss = nn.MSELoss().to(device)\n",
        "\n",
        "## Optimizer 설정하기\n",
        "# adam optimizer사용\n",
        "optim = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "\n",
        "#scheduler\n",
        "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer=optim,\n",
        "                                        lr_lambda=lambda epoch: 0.95 ** epoch,\n",
        "                                        last_epoch=-1,\n",
        "                                        verbose=False)\n",
        "\n",
        "\n",
        "\n",
        "## 그밖에 부수적인 functions 설정하기\n",
        "# output을 저장하기 위해 필요한 몇가지 함수들\n",
        "\n",
        "\n",
        "# tensor에서 numpy로 변환하는 함수\n",
        "fn_tonumpy = lambda x: x.to('cpu').detach().numpy().transpose(0, 2, 3, 1)\n",
        "# normalize된 data를 반대로 denomalization시키는 함수\n",
        "fn_denorm = lambda x, mean, std: (x * std) + mean\n",
        "# 네트워크 output의 이미지를 binary class로 분류해주는 함수\n",
        "fn_class = lambda x: 1.0 * (x > 0.5)\n",
        "\n",
        "cmap = None\n",
        "\n",
        "\n",
        "## Tensorboard 를 사용하기 위한 SummaryWriter 설정\n",
        "writer_train = SummaryWriter(log_dir=os.path.join(log_dir, 'train'))\n",
        "writer_val: SummaryWriter = SummaryWriter(log_dir=os.path.join(log_dir, 'val'))\n",
        "\n",
        "\n",
        "## 네트워크 학습시키기\n",
        "# training이 시작되는 epoch의 시작점 0으로 세팅\n",
        "st_epoch = 0\n",
        "\n",
        "\n",
        "# TRAIN \n",
        "if train_continue == \"on\":\n",
        "    net, optim, st_epoch = load(ckpt_dir=ckpt_dir, net=net, optim=optim)\n",
        "    #사전에 저장이 된 network가 있다면 연속적으로 학습하기 위해 불러와서 사용\n",
        "\n",
        "\n",
        "# training을 한다고 network에 알림림\n",
        "for epoch in range(st_epoch + 1, num_epoch + 1):\n",
        "    net.train()\n",
        "    loss_list = []\n",
        "    loss_mse_list = []\n",
        "\n",
        "\n",
        "    # network이 input을 받아 output을 출력하는 forward pass\n",
        "    for batch, data in enumerate(loader_train, 1):\n",
        "        # forward pass\n",
        "        label = data['img'].to(device)\n",
        "        noise = data['noise'].to(device)\n",
        "\n",
        "        model_input = label + noise\n",
        "        input = torch.clamp(model_input, 0, 1)\n",
        "\n",
        "        #normalization\n",
        "        input = (input - 0.5) / 0.5\n",
        "        label = (label - 0.5) / 0.5\n",
        "\n",
        "        output = net(input)\n",
        "\n",
        "\n",
        "        # backpropagation을 한느 부분\n",
        "        # backward pass\n",
        "        optim.zero_grad()\n",
        "\n",
        "        #print(output.shape)\n",
        "        #print(label.shape)\n",
        "\n",
        "        loss = fn_loss(output, label)\n",
        "        loss_list +=[loss.item()]\n",
        "        loss.backward()\n",
        "\n",
        "        optim.step()\n",
        "        scheduler.step() \n",
        "\n",
        "        \n",
        "        # mse loss 계산\n",
        "        loss_mse = mse_loss(output, label)\n",
        "        loss_mse_list += [loss_mse.item()]\n",
        "\n",
        "        print(\"TRAIN: EPOCH %04d / %04d | BATCH %04d / %04d | L1+MS-SSIM LOSS %.4f | MSE LOSS %.4f | ACCURACY %.4f\" %\n",
        "                  (epoch, num_epoch, batch, num_batch_train, np.mean(loss_list), np.mean(loss_mse_list), 1-np.mean(loss_mse_list)))\n",
        "\n",
        "        if batch % 10 == 0:\n",
        "          # Tensorboard 저장하기\n",
        "          # Tensorboard에 input, output, label을 저장\n",
        "          label = fn_tonumpy(fn_denorm(label, mean=0.5, std=0.5))\n",
        "          input = fn_tonumpy(fn_denorm(input, mean=0.5, std=0.5))\n",
        "          output = fn_tonumpy(fn_denorm(output, mean=0.5, std=0.5))\n",
        "\n",
        "          input = np.clip(input, a_min=0, a_max=1)\n",
        "          output = np.clip(output, a_min=0, a_max=1)\n",
        "\n",
        "          id = num_batch_train * (epoch - 1) + batch\n",
        "\n",
        "          plt.imsave(os.path.join(result_dir_train, 'png', '%04d_label.png' % id), label[0].squeeze(), cmap=cmap)\n",
        "          plt.imsave(os.path.join(result_dir_train, 'png', '%04d_input.png' % id), input[0].squeeze(), cmap=cmap)\n",
        "          plt.imsave(os.path.join(result_dir_train, 'png', '%04d_output.png' % id), output[0].squeeze(), cmap=cmap)\n",
        "\n",
        "\n",
        "    # loss 를 tensorboard에 저장\n",
        "    writer_train.add_scalar('loss', np.mean(loss_mse_list), epoch)\n",
        "    \n",
        "#=============================================================================네트워크를 training하는 부분 끝\n",
        "    # network validation하는 부분\n",
        "    # validatoin부분은 backpropagation이 없기에 이를 사전에 막기 위해 torch.no_grad()를 activate 시킨다.\n",
        "    # network에게 현재 validatoin한다는 것을 알리기 위해 net.eval() 활성화\n",
        "    with torch.no_grad():\n",
        "        net.eval()\n",
        "        loss_list = []\n",
        "        loss_mse_list = []\n",
        "\n",
        "        # training과 마찬가지로 forward pass진행\n",
        "        for batch, data in enumerate(loader_val, 1):\n",
        "            # forward pass\n",
        "            label = data['img'].to(device)\n",
        "            noise = data['noise'].to(device)\n",
        "\n",
        "            model_input = label + noise\n",
        "            input = torch.clamp(model_input, 0, 1)\n",
        "\n",
        "            #normalization\n",
        "            input = (input - 0.5) / 0.5\n",
        "            label = (label - 0.5) / 0.5\n",
        "\n",
        "\n",
        "            output = net(input)\n",
        "\n",
        "\n",
        "            # 손실함수 계산하기\n",
        "            loss = fn_loss(output, label)\n",
        "\n",
        "            # 손실함수 계산\n",
        "            loss_list += [loss.item()]\n",
        "\n",
        "            # mse loss 계산\n",
        "            loss_mse = mse_loss(output, label)\n",
        "            loss_mse_list += [loss_mse.item()]\n",
        "            #loss_psnr = psnr(np.mean(loss_mse))\n",
        "            print(\"VALID: EPOCH %04d / %04d | BATCH %04d / %04d | L1+MS-SSIM LOSS %.4f | MSE LOSS %.4f | ACCURACY %.4f\" %\n",
        "                  (epoch, num_epoch, batch, num_batch_train, np.mean(loss_list), np.mean(loss_mse_list), 1-np.mean(loss_mse_list)))\n",
        "            \n",
        "            if batch % 3 == 0:\n",
        "              label = fn_tonumpy(fn_denorm(label, mean=0.5, std=0.5))\n",
        "              input = fn_tonumpy(fn_denorm(input, mean=0.5, std=0.5))\n",
        "              output = fn_tonumpy(fn_denorm(output, mean=0.5, std=0.5))\n",
        "\n",
        "              input = np.clip(input, a_min=0, a_max=1)\n",
        "              output = np.clip(output, a_min=0, a_max=1)\n",
        "\n",
        "              id = num_batch_val * (epoch - 1) + batch\n",
        "\n",
        "              #결과를 png파일로 저장\n",
        "              plt.imsave(os.path.join(result_dir_val, 'png', '%04d_label.png' % id), label[0].squeeze(), cmap=cmap)\n",
        "              plt.imsave(os.path.join(result_dir_val, 'png', '%04d_input.png' % id), input[0].squeeze(), cmap=cmap)\n",
        "              plt.imsave(os.path.join(result_dir_val, 'png', '%04d_output.png' % id), output[0].squeeze(), cmap=cmap)\n",
        "\n",
        "\n",
        "    writer_val.add_scalar('loss', np.mean(loss_mse_list), epoch)\n",
        "\n",
        "    # 20번마다 한번씩 network저장\n",
        "    if epoch % 20 == 0:\n",
        "      # epoch이 진행 될 때마다 network저장\n",
        "        save(ckpt_dir=ckpt_dir, net=net, optim=optim, epoch=epoch)\n",
        "\n",
        "writer_train.close()\n",
        "writer_val.close()\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch_msssim in /usr/local/lib/python3.7/dist-packages (0.2.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from pytorch_msssim) (1.8.1+cu101)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->pytorch_msssim) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch->pytorch_msssim) (1.19.5)\n",
            "learning rate: 1.0000e-03\n",
            "batch size: 50\n",
            "number of epoch: 100\n",
            "task: denoising\n",
            "opts: ['random', 4]\n",
            "learning type: plain\n",
            "ckpt dir: /content/drive/MyDrive/Multimedia/Test1/checkpoint/\n",
            "log dir: /content/drive/MyDrive/Multimedia/Test1/log\n",
            "result dir: /content/drive/MyDrive/Multimedia/Test1/result\n",
            "device: cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0001 / 0090 | L1+MS-SSIM LOSS 86.2828 | MSE LOSS 0.3194 | ACCURACY 0.6806\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0002 / 0090 | L1+MS-SSIM LOSS 67.8019 | MSE LOSS 0.2139 | ACCURACY 0.7861\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0003 / 0090 | L1+MS-SSIM LOSS 60.9910 | MSE LOSS 0.1734 | ACCURACY 0.8266\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0004 / 0090 | L1+MS-SSIM LOSS 54.4147 | MSE LOSS 0.1419 | ACCURACY 0.8581\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0005 / 0090 | L1+MS-SSIM LOSS 49.6798 | MSE LOSS 0.1208 | ACCURACY 0.8792\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0006 / 0090 | L1+MS-SSIM LOSS 46.3337 | MSE LOSS 0.1062 | ACCURACY 0.8938\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0007 / 0090 | L1+MS-SSIM LOSS 43.5131 | MSE LOSS 0.0948 | ACCURACY 0.9052\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0008 / 0090 | L1+MS-SSIM LOSS 41.0299 | MSE LOSS 0.0856 | ACCURACY 0.9144\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0009 / 0090 | L1+MS-SSIM LOSS 39.0186 | MSE LOSS 0.0783 | ACCURACY 0.9217\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0010 / 0090 | L1+MS-SSIM LOSS 37.5039 | MSE LOSS 0.0726 | ACCURACY 0.9274\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0011 / 0090 | L1+MS-SSIM LOSS 36.2354 | MSE LOSS 0.0678 | ACCURACY 0.9322\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0012 / 0090 | L1+MS-SSIM LOSS 35.0456 | MSE LOSS 0.0636 | ACCURACY 0.9364\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0013 / 0090 | L1+MS-SSIM LOSS 33.9939 | MSE LOSS 0.0601 | ACCURACY 0.9399\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0014 / 0090 | L1+MS-SSIM LOSS 33.1102 | MSE LOSS 0.0571 | ACCURACY 0.9429\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0015 / 0090 | L1+MS-SSIM LOSS 32.3140 | MSE LOSS 0.0544 | ACCURACY 0.9456\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0016 / 0090 | L1+MS-SSIM LOSS 31.6899 | MSE LOSS 0.0521 | ACCURACY 0.9479\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0017 / 0090 | L1+MS-SSIM LOSS 31.0869 | MSE LOSS 0.0500 | ACCURACY 0.9500\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0018 / 0090 | L1+MS-SSIM LOSS 30.5491 | MSE LOSS 0.0482 | ACCURACY 0.9518\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0019 / 0090 | L1+MS-SSIM LOSS 30.0749 | MSE LOSS 0.0466 | ACCURACY 0.9534\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0020 / 0090 | L1+MS-SSIM LOSS 29.5040 | MSE LOSS 0.0449 | ACCURACY 0.9551\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0021 / 0090 | L1+MS-SSIM LOSS 29.0955 | MSE LOSS 0.0435 | ACCURACY 0.9565\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0022 / 0090 | L1+MS-SSIM LOSS 28.6832 | MSE LOSS 0.0421 | ACCURACY 0.9579\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0023 / 0090 | L1+MS-SSIM LOSS 28.5080 | MSE LOSS 0.0412 | ACCURACY 0.9588\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0024 / 0090 | L1+MS-SSIM LOSS 28.2934 | MSE LOSS 0.0403 | ACCURACY 0.9597\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0025 / 0090 | L1+MS-SSIM LOSS 27.9585 | MSE LOSS 0.0393 | ACCURACY 0.9607\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0026 / 0090 | L1+MS-SSIM LOSS 27.6320 | MSE LOSS 0.0383 | ACCURACY 0.9617\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0027 / 0090 | L1+MS-SSIM LOSS 27.3562 | MSE LOSS 0.0374 | ACCURACY 0.9626\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0028 / 0090 | L1+MS-SSIM LOSS 27.1818 | MSE LOSS 0.0367 | ACCURACY 0.9633\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0029 / 0090 | L1+MS-SSIM LOSS 26.9556 | MSE LOSS 0.0359 | ACCURACY 0.9641\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0030 / 0090 | L1+MS-SSIM LOSS 26.7621 | MSE LOSS 0.0353 | ACCURACY 0.9647\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0031 / 0090 | L1+MS-SSIM LOSS 26.5658 | MSE LOSS 0.0346 | ACCURACY 0.9654\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0032 / 0090 | L1+MS-SSIM LOSS 26.3382 | MSE LOSS 0.0340 | ACCURACY 0.9660\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0033 / 0090 | L1+MS-SSIM LOSS 26.2279 | MSE LOSS 0.0335 | ACCURACY 0.9665\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0034 / 0090 | L1+MS-SSIM LOSS 25.9832 | MSE LOSS 0.0329 | ACCURACY 0.9671\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0035 / 0090 | L1+MS-SSIM LOSS 25.7345 | MSE LOSS 0.0322 | ACCURACY 0.9678\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0036 / 0090 | L1+MS-SSIM LOSS 25.5645 | MSE LOSS 0.0317 | ACCURACY 0.9683\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0037 / 0090 | L1+MS-SSIM LOSS 25.3888 | MSE LOSS 0.0313 | ACCURACY 0.9687\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0038 / 0090 | L1+MS-SSIM LOSS 25.2032 | MSE LOSS 0.0307 | ACCURACY 0.9693\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0039 / 0090 | L1+MS-SSIM LOSS 25.0036 | MSE LOSS 0.0302 | ACCURACY 0.9698\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0040 / 0090 | L1+MS-SSIM LOSS 24.8370 | MSE LOSS 0.0298 | ACCURACY 0.9702\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0041 / 0090 | L1+MS-SSIM LOSS 24.6765 | MSE LOSS 0.0293 | ACCURACY 0.9707\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0042 / 0090 | L1+MS-SSIM LOSS 24.4872 | MSE LOSS 0.0289 | ACCURACY 0.9711\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0043 / 0090 | L1+MS-SSIM LOSS 24.3335 | MSE LOSS 0.0285 | ACCURACY 0.9715\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0044 / 0090 | L1+MS-SSIM LOSS 24.2109 | MSE LOSS 0.0281 | ACCURACY 0.9719\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0045 / 0090 | L1+MS-SSIM LOSS 24.0625 | MSE LOSS 0.0277 | ACCURACY 0.9723\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0046 / 0090 | L1+MS-SSIM LOSS 23.9268 | MSE LOSS 0.0274 | ACCURACY 0.9726\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0047 / 0090 | L1+MS-SSIM LOSS 23.8553 | MSE LOSS 0.0271 | ACCURACY 0.9729\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0048 / 0090 | L1+MS-SSIM LOSS 23.8402 | MSE LOSS 0.0269 | ACCURACY 0.9731\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0049 / 0090 | L1+MS-SSIM LOSS 23.7202 | MSE LOSS 0.0266 | ACCURACY 0.9734\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0050 / 0090 | L1+MS-SSIM LOSS 23.6046 | MSE LOSS 0.0263 | ACCURACY 0.9737\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0051 / 0090 | L1+MS-SSIM LOSS 23.4883 | MSE LOSS 0.0260 | ACCURACY 0.9740\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0052 / 0090 | L1+MS-SSIM LOSS 23.4066 | MSE LOSS 0.0258 | ACCURACY 0.9742\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0053 / 0090 | L1+MS-SSIM LOSS 23.2996 | MSE LOSS 0.0255 | ACCURACY 0.9745\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0054 / 0090 | L1+MS-SSIM LOSS 23.1992 | MSE LOSS 0.0252 | ACCURACY 0.9748\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0055 / 0090 | L1+MS-SSIM LOSS 23.1497 | MSE LOSS 0.0251 | ACCURACY 0.9749\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0056 / 0090 | L1+MS-SSIM LOSS 23.0857 | MSE LOSS 0.0248 | ACCURACY 0.9752\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0057 / 0090 | L1+MS-SSIM LOSS 23.0314 | MSE LOSS 0.0246 | ACCURACY 0.9754\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0058 / 0090 | L1+MS-SSIM LOSS 22.9437 | MSE LOSS 0.0244 | ACCURACY 0.9756\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0059 / 0090 | L1+MS-SSIM LOSS 22.8824 | MSE LOSS 0.0242 | ACCURACY 0.9758\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0060 / 0090 | L1+MS-SSIM LOSS 22.8353 | MSE LOSS 0.0241 | ACCURACY 0.9759\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0061 / 0090 | L1+MS-SSIM LOSS 22.7507 | MSE LOSS 0.0239 | ACCURACY 0.9761\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0062 / 0090 | L1+MS-SSIM LOSS 22.6698 | MSE LOSS 0.0236 | ACCURACY 0.9764\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0063 / 0090 | L1+MS-SSIM LOSS 22.6181 | MSE LOSS 0.0235 | ACCURACY 0.9765\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0064 / 0090 | L1+MS-SSIM LOSS 22.5423 | MSE LOSS 0.0233 | ACCURACY 0.9767\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0065 / 0090 | L1+MS-SSIM LOSS 22.4697 | MSE LOSS 0.0231 | ACCURACY 0.9769\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0066 / 0090 | L1+MS-SSIM LOSS 22.4279 | MSE LOSS 0.0229 | ACCURACY 0.9771\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0067 / 0090 | L1+MS-SSIM LOSS 22.3735 | MSE LOSS 0.0228 | ACCURACY 0.9772\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0068 / 0090 | L1+MS-SSIM LOSS 22.3483 | MSE LOSS 0.0227 | ACCURACY 0.9773\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0069 / 0090 | L1+MS-SSIM LOSS 22.2680 | MSE LOSS 0.0225 | ACCURACY 0.9775\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0070 / 0090 | L1+MS-SSIM LOSS 22.2038 | MSE LOSS 0.0223 | ACCURACY 0.9777\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0071 / 0090 | L1+MS-SSIM LOSS 22.1639 | MSE LOSS 0.0222 | ACCURACY 0.9778\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0072 / 0090 | L1+MS-SSIM LOSS 22.1010 | MSE LOSS 0.0221 | ACCURACY 0.9779\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0073 / 0090 | L1+MS-SSIM LOSS 22.0591 | MSE LOSS 0.0219 | ACCURACY 0.9781\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0074 / 0090 | L1+MS-SSIM LOSS 21.9915 | MSE LOSS 0.0218 | ACCURACY 0.9782\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0075 / 0090 | L1+MS-SSIM LOSS 21.9578 | MSE LOSS 0.0217 | ACCURACY 0.9783\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0076 / 0090 | L1+MS-SSIM LOSS 21.9217 | MSE LOSS 0.0215 | ACCURACY 0.9785\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0077 / 0090 | L1+MS-SSIM LOSS 21.9013 | MSE LOSS 0.0215 | ACCURACY 0.9785\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0078 / 0090 | L1+MS-SSIM LOSS 21.8789 | MSE LOSS 0.0214 | ACCURACY 0.9786\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0079 / 0090 | L1+MS-SSIM LOSS 21.8147 | MSE LOSS 0.0212 | ACCURACY 0.9788\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0080 / 0090 | L1+MS-SSIM LOSS 21.7546 | MSE LOSS 0.0211 | ACCURACY 0.9789\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0081 / 0090 | L1+MS-SSIM LOSS 21.6932 | MSE LOSS 0.0209 | ACCURACY 0.9791\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0082 / 0090 | L1+MS-SSIM LOSS 21.6628 | MSE LOSS 0.0208 | ACCURACY 0.9792\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0083 / 0090 | L1+MS-SSIM LOSS 21.6225 | MSE LOSS 0.0207 | ACCURACY 0.9793\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0084 / 0090 | L1+MS-SSIM LOSS 21.5610 | MSE LOSS 0.0206 | ACCURACY 0.9794\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0085 / 0090 | L1+MS-SSIM LOSS 21.5030 | MSE LOSS 0.0205 | ACCURACY 0.9795\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0086 / 0090 | L1+MS-SSIM LOSS 21.4984 | MSE LOSS 0.0204 | ACCURACY 0.9796\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0087 / 0090 | L1+MS-SSIM LOSS 21.4979 | MSE LOSS 0.0204 | ACCURACY 0.9796\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0088 / 0090 | L1+MS-SSIM LOSS 21.4575 | MSE LOSS 0.0203 | ACCURACY 0.9797\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0089 / 0090 | L1+MS-SSIM LOSS 21.4205 | MSE LOSS 0.0202 | ACCURACY 0.9798\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0090 / 0090 | L1+MS-SSIM LOSS 21.3870 | MSE LOSS 0.0201 | ACCURACY 0.9799\n",
            "VALID: EPOCH 0001 / 0100 | BATCH 0001 / 0090 | L1+MS-SSIM LOSS 17.2168 | MSE LOSS 0.0113 | ACCURACY 0.9887\n",
            "VALID: EPOCH 0001 / 0100 | BATCH 0002 / 0090 | L1+MS-SSIM LOSS 16.8219 | MSE LOSS 0.0107 | ACCURACY 0.9893\n",
            "VALID: EPOCH 0001 / 0100 | BATCH 0003 / 0090 | L1+MS-SSIM LOSS 16.6994 | MSE LOSS 0.0103 | ACCURACY 0.9897\n",
            "VALID: EPOCH 0001 / 0100 | BATCH 0004 / 0090 | L1+MS-SSIM LOSS 16.6395 | MSE LOSS 0.0101 | ACCURACY 0.9899\n",
            "VALID: EPOCH 0001 / 0100 | BATCH 0005 / 0090 | L1+MS-SSIM LOSS 16.6585 | MSE LOSS 0.0102 | ACCURACY 0.9898\n",
            "VALID: EPOCH 0001 / 0100 | BATCH 0006 / 0090 | L1+MS-SSIM LOSS 16.7280 | MSE LOSS 0.0102 | ACCURACY 0.9898\n",
            "VALID: EPOCH 0001 / 0100 | BATCH 0007 / 0090 | L1+MS-SSIM LOSS 16.6755 | MSE LOSS 0.0102 | ACCURACY 0.9898\n",
            "VALID: EPOCH 0001 / 0100 | BATCH 0008 / 0090 | L1+MS-SSIM LOSS 16.6418 | MSE LOSS 0.0101 | ACCURACY 0.9899\n",
            "VALID: EPOCH 0001 / 0100 | BATCH 0009 / 0090 | L1+MS-SSIM LOSS 16.6738 | MSE LOSS 0.0102 | ACCURACY 0.9898\n",
            "VALID: EPOCH 0001 / 0100 | BATCH 0010 / 0090 | L1+MS-SSIM LOSS 16.6789 | MSE LOSS 0.0102 | ACCURACY 0.9898\n",
            "TRAIN: EPOCH 0002 / 0100 | BATCH 0001 / 0090 | L1+MS-SSIM LOSS 17.2302 | MSE LOSS 0.0108 | ACCURACY 0.9892\n",
            "TRAIN: EPOCH 0002 / 0100 | BATCH 0002 / 0090 | L1+MS-SSIM LOSS 17.8509 | MSE LOSS 0.0113 | ACCURACY 0.9887\n",
            "TRAIN: EPOCH 0002 / 0100 | BATCH 0003 / 0090 | L1+MS-SSIM LOSS 18.2605 | MSE LOSS 0.0118 | ACCURACY 0.9882\n",
            "TRAIN: EPOCH 0002 / 0100 | BATCH 0004 / 0090 | L1+MS-SSIM LOSS 18.2110 | MSE LOSS 0.0116 | ACCURACY 0.9884\n",
            "TRAIN: EPOCH 0002 / 0100 | BATCH 0005 / 0090 | L1+MS-SSIM LOSS 18.1796 | MSE LOSS 0.0115 | ACCURACY 0.9885\n",
            "TRAIN: EPOCH 0002 / 0100 | BATCH 0006 / 0090 | L1+MS-SSIM LOSS 17.9976 | MSE LOSS 0.0113 | ACCURACY 0.9887\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-c9bec176b8d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;31m# mse loss 계산\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0mloss_mse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0mloss_mse_list\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mloss_mse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         print(\"TRAIN: EPOCH %04d / %04d | BATCH %04d / %04d | L1+MS-SSIM LOSS %.4f | MSE LOSS %.4f | ACCURACY %.4f\" %\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-k_5CSnI9Ip"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOzB_NQqI_eN"
      },
      "source": [
        "import argparse\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torchvision import transforms\n",
        "\n",
        "ckpt_dir = \"/content/drive/MyDrive/Multimedia/Termproject/checkpoint/\"\n",
        "result_dir = \"/content/drive/MyDrive/Multimedia/Test1/result\"\n",
        "\n",
        "## 디렉토리 생성하기\n",
        "result_dir_test = os.path.join(result_dir, 'test')\n",
        "if not os.path.exists(result_dir):\n",
        "    os.makedirs(os.path.join(result_dir_test, 'png'))\n",
        "    os.makedirs(os.path.join(result_dir_test, 'numpy'))\n",
        "\n",
        "\n",
        "dataset_test = NoiseDataset(root_path, 128) #128은 size\n",
        "dataset_test.set_mode(\"testing\")\n",
        "loader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "# 그밖에 부수적인 variables 설정하기\n",
        "num_data_test = len(dataset_test)\n",
        "num_batch_test = np.ceil(num_data_test / batch_size)\n",
        "\n",
        "\n",
        "## 손실함수 정의하기\n",
        "# fn_loss = nn.BCEWithLogitsLoss().to(device)\n",
        "fn_loss = nn.MSELoss().to(device)\n",
        "\n",
        "\n",
        "## 네트워크 생성하기\n",
        "net = UNet(nch=nch, nker=nker, learning_type=learning_type).to(device)\n",
        "\n",
        "\n",
        "## Optimizer 설정하기\n",
        "# adam optimizer사용\n",
        "optim = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "\n",
        "\n",
        "## 그밖에 부수적인 functions 설정하기\n",
        "# output을 저장하기 위해 필요한 몇가지 함수들\n",
        "\n",
        "\n",
        "# tensor에서 numpy로 변환하는 함수\n",
        "fn_tonumpy = lambda x: x.to('cpu').detach().numpy().transpose(0, 2, 3, 1)\n",
        "# normalize된 data를 반대로 denomalization시키는 함수\n",
        "fn_denorm = lambda x, mean, std: (x * std) + mean\n",
        "# 네트워크 output의 이미지를 binary class로 분류해주는 함수\n",
        "fn_class = lambda x: 1.0 * (x > 0.5)\n",
        "\n",
        "cmap = None\n",
        "\n",
        "net, optim, st_epoch = load(ckpt_dir=ckpt_dir, net=net, optim=optim)\n",
        "\n",
        "\n",
        "# backpropagation이 없기에 이를 사전에 막기 위해 torch.no_grad()를 activate 시킨다.\n",
        "# network에게 현재 validatoin한다는 것을 알리기 위해 net.eval() 활성화\n",
        "with torch.no_grad():\n",
        "    net.eval()\n",
        "    loss_mse = []\n",
        "\n",
        "    for batch, data in enumerate(loader_test, 1):\n",
        "        # forward pass\n",
        "        input = data['img'].to(device)\n",
        "\n",
        "        #normalization\n",
        "        input = (input - 0.5) / 0.5\n",
        "\n",
        "        output = net(input)\n",
        "\n",
        "\n",
        "        print(\"TEST: BATCH %04d / %04d\" %\n",
        "              (batch, num_batch_test))\n",
        "        \n",
        "\n",
        "        # Tensorboard 저장하기\n",
        "        input = fn_tonumpy(fn_denorm(input, mean=0.5, std=0.5))\n",
        "        output = fn_tonumpy(fn_denorm(output, mean=0.5, std=0.5))\n",
        "\n",
        "        for j in range(input.shape[0]):\n",
        "            id = batch_size * (batch - 1) + j\n",
        "\n",
        "            input_ = input[j]\n",
        "            output_ = output[j]\n",
        "\n",
        "            # 결과를 png파일로 저장장\n",
        "            input_ = np.clip(input_, a_min=0, a_max=1)\n",
        "            output_ = np.clip(output_, a_min=0, a_max=1)\n",
        "\n",
        "            plt.imsave(os.path.join(result_dir_test, 'png', '%04d_input.png' % id), input_, cmap=cmap)\n",
        "            plt.imsave(os.path.join(result_dir_test, 'png', '%04d_output.png' % id), output_, cmap=cmap)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}