{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "N3_Multimedia_Termproject.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BQmYo2rqmK4"
      },
      "source": [
        "# Get Dataset from Google Drive  \n",
        "Please upload your dataset on google drive first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vxHB3wfW911",
        "outputId": "3e2369aa-cb66-4c0e-9a94-29febca49133"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbDhYfKKa41s"
      },
      "source": [
        "import os\n",
        "import zipfile\n",
        "import tqdm\n",
        "\n",
        "file_name = \"Multimedia_dataset.zip\"\n",
        "zip_path = os.path.join('/content/drive/MyDrive/Multimedia/Multimedia_dataset/Multimedia_dataset.zip')\n",
        "\n",
        "!cp \"{zip_path}\" .\n",
        "!unzip -q \"{file_name}\"\n",
        "!rm \"{file_name}\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmbp65jgq67j"
      },
      "source": [
        "# Noise Transform  \n",
        "If you want to change how much noise you are giving, change the stddev and mean values at 'gaussian_noise' function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyeUBdtKYQSu"
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torchvision import transforms\n",
        "\n",
        "import random\n",
        "\n",
        "class NoiseTransform(object):\n",
        "  def __init__(self, size=180, mode=\"training\"):\n",
        "    super(NoiseTransform, self).__init__()\n",
        "    self.size = size\n",
        "    self.mode = mode\n",
        "  \n",
        "  def gaussian_noise(self, img):\n",
        "    mean = 0\n",
        "    stddev = 25\n",
        "    noise = Variable(torch.zeros(img.size()))\n",
        "    noise = noise.data.normal_(mean, stddev/255.)\n",
        "\n",
        "    return noise\n",
        "\n",
        "  def __call__(self, img):\n",
        "    if (self.mode == \"training\") | (self.mode == \"validation\"):\n",
        "      self.gt_transform = transforms.Compose([\n",
        "        transforms.Resize((self.size, self.size), interpolation=2),\n",
        "        transforms.ToTensor()])\n",
        "      self.noise_transform = transforms.Compose([\n",
        "        transforms.Resize((self.size, self.size), interpolation=2),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Lambda(self.gaussian_noise),\n",
        "      ])\n",
        "      return self.gt_transform(img), self.noise_transform(img)\n",
        "\n",
        "    elif self.mode == \"testing\":\n",
        "      self.gt_transform = transforms.Compose([\n",
        "        transforms.ToTensor()])\n",
        "      return self.gt_transform(img)\n",
        "    else:\n",
        "      return NotImplementedError\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGKbE7uFrWwb"
      },
      "source": [
        "# Dataloader for Noise Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxZsXXZ9YpYp"
      },
      "source": [
        "import torch\n",
        "import torch.utils.data  as data\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class NoiseDataset(data.Dataset):\n",
        "  def __init__(self, root_path, size):\n",
        "    super(NoiseDataset, self).__init__()\n",
        "\n",
        "    self.root_path = root_path\n",
        "    self.size = size\n",
        "    self.transforms = None\n",
        "    self.examples = None\n",
        "\n",
        "  def set_mode(self, mode):\n",
        "    self.mode = mode\n",
        "    self.transforms = NoiseTransform(self.size, mode)\n",
        "    if mode == \"training\":\n",
        "      train_dir = os.path.join(self.root_path, \"train\")\n",
        "      self.examples = [os.path.join(self.root_path, \"train\", dirs) for dirs in os.listdir(train_dir)]\n",
        "    elif mode == \"validation\":\n",
        "      val_dir = os.path.join(self.root_path, \"validation\")\n",
        "      self.examples = [os.path.join(self.root_path, \"validation\", dirs) for dirs in os.listdir(val_dir)]\n",
        "    elif mode == \"testing\":\n",
        "      test_dir = os.path.join(self.root_path, \"test\")\n",
        "      self.examples = [os.path.join(self.root_path, \"test\", dirs) for dirs in os.listdir(test_dir)]\n",
        "    else:\n",
        "      raise NotImplementedError\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.examples)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    file_name = self.examples[idx]\n",
        "    image = Image.open(file_name)\n",
        "\n",
        "\n",
        "    if self.mode == \"testing\":\n",
        "      input_img = self.transforms(image)\n",
        "      sample = {\"img\": input_img}\n",
        "    else:\n",
        "      clean, noise = self.transforms(image)\n",
        "      sample = {\"img\": clean, \"noise\": noise}\n",
        "\n",
        "    return sample"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDx4vFU-rf5z"
      },
      "source": [
        "# Example for Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53tDtBFLenTz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "b931308d-14f8-4de5-8dc1-ae9eae727aae"
      },
      "source": [
        "import torch\n",
        "import torch.utils.data  as data\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms\n",
        "import tqdm\n",
        "from PIL import Image\n",
        "\n",
        "def image_show(img):\n",
        "  if isinstance(img, torch.Tensor):\n",
        "    # PIL image로 바꿔준다.\n",
        "    img = transforms.ToPILImage()(img)\n",
        "  plt.imshow(img)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# Change to your data root directory\n",
        "root_path = \"/content/\"\n",
        "# Depend on runtime setting\n",
        "use_cuda = True\n",
        "\n",
        "train_dataset = NoiseDataset(root_path, 128) #128은 size\n",
        "train_dataset.set_mode(\"training\")\n",
        "\n",
        "\n",
        "# batch=4 단위로 data를 load\n",
        "train_dataloader = data.DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "\"\"\"\n",
        "# tqdm은 진행표시바\n",
        "for i, data in enumerate(tqdm.tqdm(train_dataloader)):\n",
        "  #CUDA는 NVIDIA에서 개발한 GPU 개발 툴로 많은 양의 연산을 동시에 처리하는 것이 목표\n",
        "  if use_cuda:\n",
        "    img = data[\"img\"].to('cuda')\n",
        "    noise = data[\"noise\"].to('cuda')\n",
        "  \n",
        "  model_input = img + noise\n",
        "\n",
        "  # clamp는 최대 최소 값을 정해주는 함수\n",
        "  # (최소~최대 범위에 포함되지 않는게 있으면 그 값을 최소 최대값으로 변경)\n",
        "  noise_image = torch.clamp(model_input, 0, 1)\n",
        "\n",
        "  image_show(img[0])\n",
        "  image_show(noise[0])\n",
        "\n",
        "\n",
        "\n",
        "  input()\n",
        "\"\"\""
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n# tqdm은 진행표시바\\nfor i, data in enumerate(tqdm.tqdm(train_dataloader)):\\n  #CUDA는 NVIDIA에서 개발한 GPU 개발 툴로 많은 양의 연산을 동시에 처리하는 것이 목표\\n  if use_cuda:\\n    img = data[\"img\"].to(\\'cuda\\')\\n    noise = data[\"noise\"].to(\\'cuda\\')\\n  \\n  model_input = img + noise\\n\\n  # clamp는 최대 최소 값을 정해주는 함수\\n  # (최소~최대 범위에 포함되지 않는게 있으면 그 값을 최소 최대값으로 변경)\\n  noise_image = torch.clamp(model_input, 0, 1)\\n\\n  image_show(img[0])\\n  image_show(noise[0])\\n\\n\\n\\n  input()\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtESD4NV4Wks"
      },
      "source": [
        "# UNet Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVJI37g45PLz"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "## 네트워크 구축하기\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, nch, nker=64, learning_type=\"plain\", norm=\"bnorm\"):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        self.learning_type = learning_type\n",
        "\n",
        "        # Contracting path\n",
        "        self.enc1_1 = CBR2d(in_channels=nch, out_channels=1 * nker, norm=norm)\n",
        "        self.enc1_2 = CBR2d(in_channels=1 * nker, out_channels=1 * nker, norm=norm)\n",
        "\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.enc2_1 = CBR2d(in_channels=nker, out_channels=2 * nker, norm=norm)\n",
        "        self.enc2_2 = CBR2d(in_channels=2 * nker, out_channels=2 * nker, norm=norm)\n",
        "\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.enc3_1 = CBR2d(in_channels=2 * nker, out_channels=4 * nker, norm=norm)\n",
        "        self.enc3_2 = CBR2d(in_channels=4 * nker, out_channels=4 * nker, norm=norm)\n",
        "\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.enc4_1 = CBR2d(in_channels=4 * nker, out_channels=8 * nker, norm=norm)\n",
        "        self.enc4_2 = CBR2d(in_channels=8 * nker, out_channels=8 * nker, norm=norm)\n",
        "\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.enc5_1 = CBR2d(in_channels=8 * nker, out_channels=16 * nker, norm=norm)\n",
        "\n",
        "\n",
        "\n",
        "        # Expansive path\n",
        "        self.dec5_1 = CBR2d(in_channels=16 * nker, out_channels=8 * nker, norm=norm)\n",
        "\n",
        "        self.unpool4 = nn.ConvTranspose2d(in_channels=8 * nker, out_channels=8 * nker,\n",
        "                                          kernel_size=2, stride=2, padding=0, bias=True)\n",
        "\n",
        "        self.dec4_2 = CBR2d(in_channels=2 * 8 * nker, out_channels=8 * nker, norm=norm)\n",
        "        self.dec4_1 = CBR2d(in_channels=8 * nker, out_channels=4 * nker, norm=norm)\n",
        "\n",
        "        self.unpool3 = nn.ConvTranspose2d(in_channels=4 * nker, out_channels=4 * nker,\n",
        "                                          kernel_size=2, stride=2, padding=0, bias=True)\n",
        "\n",
        "        self.dec3_2 = CBR2d(in_channels=2 * 4 * nker, out_channels=4 * nker, norm=norm)\n",
        "        self.dec3_1 = CBR2d(in_channels=4 * nker, out_channels=2 * nker, norm=norm)\n",
        "\n",
        "        self.unpool2 = nn.ConvTranspose2d(in_channels=2 * nker, out_channels=2 * nker,\n",
        "                                          kernel_size=2, stride=2, padding=0, bias=True)\n",
        "\n",
        "        self.dec2_2 = CBR2d(in_channels=2 * 2 * nker, out_channels=2 * nker, norm=norm)\n",
        "        self.dec2_1 = CBR2d(in_channels=2 * nker, out_channels=1 * nker, norm=norm)\n",
        "\n",
        "        self.unpool1 = nn.ConvTranspose2d(in_channels=1 * nker, out_channels=1 * nker,\n",
        "                                          kernel_size=2, stride=2, padding=0, bias=True)\n",
        "\n",
        "        self.dec1_2 = CBR2d(in_channels=2 * 1 * nker, out_channels=1 * nker, norm=norm)\n",
        "        self.dec1_1 = CBR2d(in_channels=1 * nker, out_channels=1 * nker, norm=norm)\n",
        "\n",
        "        self.fc = nn.Conv2d(in_channels=1 * nker, out_channels=nch, kernel_size=1, stride=1, padding=0, bias=True)\n",
        "\n",
        "\n",
        "    # Unet Layer 연결 (Forwarding)\n",
        "    # 위에서 정의한 것을 순서대로 실행한다고 생각하면 됨\n",
        "    def forward(self, x):\n",
        "        # forward encoder\n",
        "        enc1_1 = self.enc1_1(x)\n",
        "        enc1_2 = self.enc1_2(enc1_1)\n",
        "        pool1 = self.pool1(enc1_2)\n",
        "\n",
        "        enc2_1 = self.enc2_1(pool1)\n",
        "        enc2_2 = self.enc2_2(enc2_1)\n",
        "        pool2 = self.pool2(enc2_2)\n",
        "\n",
        "        enc3_1 = self.enc3_1(pool2)\n",
        "        enc3_2 = self.enc3_2(enc3_1)\n",
        "        pool3 = self.pool3(enc3_2)\n",
        "\n",
        "        enc4_1 = self.enc4_1(pool3)\n",
        "        enc4_2 = self.enc4_2(enc4_1)\n",
        "        pool4 = self.pool4(enc4_2)\n",
        "\n",
        "        enc5_1 = self.enc5_1(pool4)\n",
        "\n",
        "        # forward decoder\n",
        "        dec5_1 = self.dec5_1(enc5_1)\n",
        "\n",
        "        unpool4 = self.unpool4(dec5_1)\n",
        "        # cat은 이전 step의 output channel과 skip connection을 연결해주는 부분\n",
        "        # dim = [0: batch, 1: channel, 2: height, 3: width] <- dim은 해당 방향을 알려준다.\n",
        "        cat4 = torch.cat((unpool4, enc4_2), dim=1)\n",
        "        dec4_2 = self.dec4_2(cat4)\n",
        "        dec4_1 = self.dec4_1(dec4_2)\n",
        "\n",
        "        unpool3 = self.unpool3(dec4_1)\n",
        "        cat3 = torch.cat((unpool3, enc3_2), dim=1)\n",
        "        dec3_2 = self.dec3_2(cat3)\n",
        "        dec3_1 = self.dec3_1(dec3_2)\n",
        "\n",
        "        unpool2 = self.unpool2(dec3_1)\n",
        "        cat2 = torch.cat((unpool2, enc2_2), dim=1)\n",
        "        dec2_2 = self.dec2_2(cat2)\n",
        "        dec2_1 = self.dec2_1(dec2_2)\n",
        "\n",
        "        unpool1 = self.unpool1(dec2_1)\n",
        "        cat1 = torch.cat((unpool1, enc1_2), dim=1)\n",
        "        dec1_2 = self.dec1_2(cat1)\n",
        "        dec1_1 = self.dec1_1(dec1_2)\n",
        "\n",
        "        if self.learning_type == \"plain\":\n",
        "            x = self.fc(dec1_1)\n",
        "        elif self.learning_type == \"residual\":\n",
        "            x = x + self.fc(dec1_1)\n",
        "\n",
        "        return x\n",
        "  \n",
        "## 네트워크 저장하기\n",
        "def save(ckpt_dir, net, optim, epoch):\n",
        "    if not os.path.exists(ckpt_dir):\n",
        "        os.makedirs(ckpt_dir)\n",
        "\n",
        "    torch.save({'net': net.state_dict(), 'optim': optim.state_dict()},\n",
        "               \"%s/model_epoch%d.pth\" % (ckpt_dir, epoch))\n",
        "\n",
        "## 네트워크 불러오기\n",
        "def load(ckpt_dir, net, optim):\n",
        "    if not os.path.exists(ckpt_dir):\n",
        "        epoch = 0\n",
        "        return net, optim, epoch\n",
        "\n",
        "    ckpt_lst = os.listdir(ckpt_dir)\n",
        "    ckpt_lst.sort(key=lambda f: int(''.join(filter(str.isdigit, f))))\n",
        "\n",
        "    dict_model = torch.load('%s/%s' % (ckpt_dir, ckpt_lst[-1]))\n",
        "\n",
        "    net.load_state_dict(dict_model['net'])\n",
        "    optim.load_state_dict(dict_model['optim'])\n",
        "    epoch = int(ckpt_lst[-1].split('epoch')[1].split('.pth')[0])\n",
        "\n",
        "    return net, optim, epoch\n",
        "\n",
        "\n",
        "class CBR2d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True, norm=\"bnorm\", relu=0.0):\n",
        "        super().__init__()\n",
        "\n",
        "        layers = []\n",
        "        layers += [nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
        "                             kernel_size=kernel_size, stride=stride, padding=padding,\n",
        "                             bias=bias)]\n",
        "\n",
        "        if not norm is None:\n",
        "            if norm == \"bnorm\":\n",
        "                layers += [nn.BatchNorm2d(num_features=out_channels)]\n",
        "            elif norm == \"inorm\":\n",
        "                layers += [nn.InstanceNorm2d(num_features=out_channels)]\n",
        "\n",
        "        if not relu is None and relu >= 0.0:\n",
        "            layers += [nn.ReLU() if relu == 0 else nn.LeakyReLU(relu)]\n",
        "\n",
        "        self.cbr = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.cbr(x)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xh1CUC654Q87"
      },
      "source": [
        "# Trainer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcjIWFPo4VeW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 534
        },
        "outputId": "d40547a1-182c-4ead-ef57-c798a3a094b6"
      },
      "source": [
        "import argparse\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "## 트레이닝 파라메터 설정하기\n",
        "train_continue = \"off\"\n",
        "\n",
        "lr = 1e-3\n",
        "batch_size = 100\n",
        "num_epoch = 100\n",
        "\n",
        "ckpt_dir = \"/content/drive/MyDrive/Multimedia/Termproject/checkpoint/\"\n",
        "log_dir = \"/content/drive/MyDrive/Multimedia/Termproject/log\"\n",
        "result_dir = \"/content/drive/MyDrive/Multimedia/Termproject/result\"\n",
        "\n",
        "task = 'denoising'\n",
        "opts = ['random', 4]\n",
        "\n",
        "\n",
        "nch = 3\n",
        "nker = 64\n",
        "\n",
        "learning_type = 'plain'\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "\n",
        "print(\"learning rate: %.4e\" % lr)\n",
        "print(\"batch size: %d\" % batch_size)\n",
        "print(\"number of epoch: %d\" % num_epoch)\n",
        "\n",
        "print(\"task: %s\" % task)\n",
        "print(\"opts: %s\" % opts)\n",
        "\n",
        "print(\"learning type: %s\" % learning_type)\n",
        "\n",
        "print(\"ckpt dir: %s\" % ckpt_dir)\n",
        "print(\"log dir: %s\" % log_dir)\n",
        "print(\"result dir: %s\" % result_dir)\n",
        "\n",
        "print(\"device: %s\" % device)\n",
        "\n",
        "## 디렉토리 생성하기\n",
        "result_dir_train = os.path.join(result_dir, 'train')\n",
        "result_dir_val = os.path.join(result_dir, 'val')\n",
        "\n",
        "if not os.path.exists(result_dir):\n",
        "    os.makedirs(os.path.join(result_dir_train, 'png'))\n",
        "    # os.makedirs(os.path.join(result_dir_train, 'numpy'))\n",
        "\n",
        "    os.makedirs(os.path.join(result_dir_val, 'png'))\n",
        "    # os.makedirs(os.path.join(result_dir_val, 'numpy'))\n",
        "\n",
        "    os.makedirs(os.path.join(result_dir_test, 'png'))\n",
        "    os.makedirs(os.path.join(result_dir_test, 'numpy'))\n",
        "\n",
        "\n",
        "# Change to your data root directory\n",
        "root_path = \"/content/\"\n",
        "# Depend on runtime setting\n",
        "use_cuda = True\n",
        "\n",
        "\n",
        "\n",
        "# training dataset 불러오기\n",
        "dataset_train = NoiseDataset(root_path, 128) #128은 size\n",
        "dataset_train.set_mode(\"training\")\n",
        "loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "\n",
        "# validation dataset불러오기\n",
        "dataset_val = NoiseDataset(root_path, 128) #128은 size\n",
        "dataset_val.set_mode(\"validation\")\n",
        "loader_val = DataLoader(dataset_val, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "\n",
        "# 그밖에 부수적인 variables 설정하기\n",
        "num_data_train = len(dataset_train)\n",
        "num_data_val = len(dataset_val)\n",
        "\n",
        "num_batch_train = np.ceil(num_data_train / batch_size)\n",
        "num_batch_val = np.ceil(num_data_val / batch_size)\n",
        "\n",
        "\n",
        "\n",
        "## 네트워크 생성하기\n",
        "net = UNet(nch=nch, nker=nker, learning_type=learning_type).to(device)\n",
        "\n",
        "\n",
        "## 손실함수 정의하기\n",
        "# fn_loss = nn.BCEWithLogitsLoss().to(device)\n",
        "fn_loss = nn.MSELoss().to(device)\n",
        "\n",
        "\n",
        "## Optimizer 설정하기\n",
        "# adam optimizer사용\n",
        "optim = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "\n",
        "\n",
        "## 그밖에 부수적인 functions 설정하기\n",
        "# output을 저장하기 위해 필요한 몇가지 함수들\n",
        "\n",
        "\n",
        "# tensor에서 numpy로 변환하는 함수\n",
        "fn_tonumpy = lambda x: x.to('cpu').detach().numpy().transpose(0, 2, 3, 1)\n",
        "# normalize된 data를 반대로 denomalization시키는 함수\n",
        "fn_denorm = lambda x, mean, std: (x * std) + mean\n",
        "# 네트워크 output의 이미지를 binary class로 분류해주는 함수\n",
        "fn_class = lambda x: 1.0 * (x > 0.5)\n",
        "\n",
        "cmap = None\n",
        "\n",
        "\n",
        "## Tensorboard 를 사용하기 위한 SummaryWriter 설정\n",
        "writer_train = SummaryWriter(log_dir=os.path.join(log_dir, 'train'))\n",
        "writer_val: SummaryWriter = SummaryWriter(log_dir=os.path.join(log_dir, 'val'))\n",
        "\n",
        "\n",
        "## 네트워크 학습시키기\n",
        "# training이 시작되는 epoch의 시작점 0으로 세팅\n",
        "st_epoch = 0\n",
        "\n",
        "\n",
        "# TRAIN \n",
        "if train_continue == \"on\":\n",
        "    net, optim, st_epoch = load(ckpt_dir=ckpt_dir, net=net, optim=optim)\n",
        "    #사전에 저장이 된 network가 있다면 연속적으로 학습하기 위해 불러와서 사용\n",
        "\n",
        "\n",
        "# training을 한다고 network에 알림림\n",
        "for epoch in range(st_epoch + 1, num_epoch + 1):\n",
        "    net.train()\n",
        "    loss_mse = []\n",
        "\n",
        "\n",
        "    # network이 input을 받아 output을 출력하는 forward pass\n",
        "    for batch, data in enumerate(loader_train, 1):\n",
        "        # forward pass\n",
        "        label = data['img'].to(device)\n",
        "        noise = data['noise'].to(device)\n",
        "\n",
        "        model_input = label + noise\n",
        "        input = torch.clamp(model_input, 0, 1)\n",
        "\n",
        "        #normalization\n",
        "        input = (input - 0.5) / 0.5\n",
        "        label = (label - 0.5) / 0.5\n",
        "\n",
        "        output = net(input)\n",
        "\n",
        "\n",
        "        # backpropagation을 한느 부분\n",
        "        # backward pass\n",
        "        optim.zero_grad()\n",
        "\n",
        "        loss = fn_loss(output, label)\n",
        "        loss.backward()\n",
        "\n",
        "        optim.step()\n",
        "\n",
        "        # 손실함수 계산\n",
        "        loss_mse += [loss.item()]\n",
        "\n",
        "        print(\"TRAIN: EPOCH %04d / %04d | BATCH %04d / %04d | LOSS %.4f | ACCURACY %.4f\" %\n",
        "              (epoch, num_epoch, batch, num_batch_train, np.mean(loss_mse), 1-np.mean(loss_mse)))\n",
        "\n",
        "        if batch % 10 == 0:\n",
        "          # Tensorboard 저장하기\n",
        "          # Tensorboard에 input, output, label을 저장\n",
        "          label = fn_tonumpy(fn_denorm(label, mean=0.5, std=0.5))\n",
        "          input = fn_tonumpy(fn_denorm(input, mean=0.5, std=0.5))\n",
        "          output = fn_tonumpy(fn_denorm(output, mean=0.5, std=0.5))\n",
        "\n",
        "          input = np.clip(input, a_min=0, a_max=1)\n",
        "          output = np.clip(output, a_min=0, a_max=1)\n",
        "\n",
        "          id = num_batch_train * (epoch - 1) + batch\n",
        "\n",
        "          plt.imsave(os.path.join(result_dir_train, 'png', '%04d_label.png' % id), label[0].squeeze(), cmap=cmap)\n",
        "          plt.imsave(os.path.join(result_dir_train, 'png', '%04d_input.png' % id), input[0].squeeze(), cmap=cmap)\n",
        "          plt.imsave(os.path.join(result_dir_train, 'png', '%04d_output.png' % id), output[0].squeeze(), cmap=cmap)\n",
        "\n",
        "\n",
        "    # loss 를 tensorboard에 저장\n",
        "    writer_train.add_scalar('loss', np.mean(loss_mse), epoch)\n",
        "    \n",
        "#=============================================================================네트워크를 training하는 부분 끝\n",
        "    # network validation하는 부분\n",
        "    # validatoin부분은 backpropagation이 없기에 이를 사전에 막기 위해 torch.no_grad()를 activate 시킨다.\n",
        "    # network에게 현재 validatoin한다는 것을 알리기 위해 net.eval() 활성화\n",
        "    with torch.no_grad():\n",
        "        net.eval()\n",
        "        loss_mse = []\n",
        "\n",
        "        # training과 마찬가지로 forward pass진행\n",
        "        for batch, data in enumerate(loader_val, 1):\n",
        "            # forward pass\n",
        "            label = data['img'].to(device)\n",
        "            noise = data['noise'].to(device)\n",
        "\n",
        "            model_input = label + noise\n",
        "            input = torch.clamp(model_input, 0, 1)\n",
        "\n",
        "            #normalization\n",
        "            input = (input - 0.5) / 0.5\n",
        "            label = (label - 0.5) / 0.5\n",
        "\n",
        "\n",
        "            output = net(input)\n",
        "\n",
        "\n",
        "            # 손실함수 계산하기\n",
        "            loss = fn_loss(output, label)\n",
        "\n",
        "            loss_mse += [loss.item()]\n",
        "\n",
        "            print(\"VALID: EPOCH %04d / %04d | BATCH %04d / %04d | LOSS %.4f | ACCURACY %.4f\" %\n",
        "                  (epoch, num_epoch, batch, num_batch_val, np.mean(loss_mse), 1-np.mean(loss_mse)))\n",
        "\n",
        "            if batch % 3 == 0:\n",
        "              # Tensorboard 저장하기\n",
        "              label = fn_tonumpy(fn_denorm(label, mean=0.5, std=0.5))\n",
        "              input = fn_tonumpy(fn_denorm(input, mean=0.5, std=0.5))\n",
        "              output = fn_tonumpy(fn_denorm(output, mean=0.5, std=0.5))\n",
        "\n",
        "              input = np.clip(input, a_min=0, a_max=1)\n",
        "              output = np.clip(output, a_min=0, a_max=1)\n",
        "\n",
        "              id = num_batch_val * (epoch - 1) + batch\n",
        "\n",
        "              #결과를 png파일로 저장\n",
        "              plt.imsave(os.path.join(result_dir_val, 'png', '%04d_label.png' % id), label[0].squeeze(), cmap=cmap)\n",
        "              plt.imsave(os.path.join(result_dir_val, 'png', '%04d_input.png' % id), input[0].squeeze(), cmap=cmap)\n",
        "              plt.imsave(os.path.join(result_dir_val, 'png', '%04d_output.png' % id), output[0].squeeze(), cmap=cmap)\n",
        "\n",
        "\n",
        "    writer_val.add_scalar('loss', np.mean(loss_mse), epoch)\n",
        "\n",
        "    # 20번마다 한번씩 network저장\n",
        "    if epoch % 20 == 0:\n",
        "      # epoch이 진행 될 때마다 network저장\n",
        "        save(ckpt_dir=ckpt_dir, net=net, optim=optim, epoch=epoch)\n",
        "\n",
        "writer_train.close()\n",
        "writer_val.close()\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "learning rate: 1.0000e-03\n",
            "batch size: 100\n",
            "number of epoch: 100\n",
            "task: denoising\n",
            "opts: ['random', 4]\n",
            "learning type: plain\n",
            "ckpt dir: /content/drive/MyDrive/Multimedia/Termproject/checkpoint/\n",
            "log dir: /content/drive/MyDrive/Multimedia/Termproject/log\n",
            "result dir: /content/drive/MyDrive/Multimedia/Termproject/result\n",
            "device: cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0001 / 0045 | LOSS 0.4089 | ACCURACY 0.5911\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0002 / 0045 | LOSS 0.2841 | ACCURACY 0.7159\n",
            "TRAIN: EPOCH 0001 / 0100 | BATCH 0003 / 0045 | LOSS 0.2166 | ACCURACY 0.7834\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-84589caa23ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;31m# 손실함수 계산\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0mloss_mse\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         print(\"TRAIN: EPOCH %04d / %04d | BATCH %04d / %04d | LOSS %.4f | ACCURACY %.4f\" %\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-k_5CSnI9Ip"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOzB_NQqI_eN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "060b1716-cf82-4a32-bde6-3b2a168799ea"
      },
      "source": [
        "import argparse\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torchvision import transforms\n",
        "\n",
        "ckpt_dir = \"/content/drive/MyDrive/Multimedia/Termproject/checkpoint/\"\n",
        "\n",
        "\n",
        "## 디렉토리 생성하기\n",
        "result_dir_test = os.path.join(result_dir, 'test')\n",
        "if not os.path.exists(result_dir):\n",
        "    os.makedirs(os.path.join(result_dir_test, 'png'))\n",
        "    os.makedirs(os.path.join(result_dir_test, 'numpy'))\n",
        "\n",
        "\n",
        "dataset_test = NoiseDataset(root_path, 128) #128은 size\n",
        "dataset_test.set_mode(\"testing\")\n",
        "loader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "# 그밖에 부수적인 variables 설정하기\n",
        "num_data_test = len(dataset_test)\n",
        "num_batch_test = np.ceil(num_data_test / batch_size)\n",
        "\n",
        "\n",
        "## 손실함수 정의하기\n",
        "# fn_loss = nn.BCEWithLogitsLoss().to(device)\n",
        "fn_loss = nn.MSELoss().to(device)\n",
        "\n",
        "\n",
        "## 네트워크 생성하기\n",
        "net = UNet(nch=nch, nker=nker, learning_type=learning_type).to(device)\n",
        "\n",
        "\n",
        "## Optimizer 설정하기\n",
        "# adam optimizer사용\n",
        "optim = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "\n",
        "\n",
        "## 그밖에 부수적인 functions 설정하기\n",
        "# output을 저장하기 위해 필요한 몇가지 함수들\n",
        "\n",
        "\n",
        "# tensor에서 numpy로 변환하는 함수\n",
        "fn_tonumpy = lambda x: x.to('cpu').detach().numpy().transpose(0, 2, 3, 1)\n",
        "# normalize된 data를 반대로 denomalization시키는 함수\n",
        "fn_denorm = lambda x, mean, std: (x * std) + mean\n",
        "# 네트워크 output의 이미지를 binary class로 분류해주는 함수\n",
        "fn_class = lambda x: 1.0 * (x > 0.5)\n",
        "\n",
        "cmap = None\n",
        "\n",
        "net, optim, st_epoch = load(ckpt_dir=ckpt_dir, net=net, optim=optim)\n",
        "\n",
        "\n",
        "# backpropagation이 없기에 이를 사전에 막기 위해 torch.no_grad()를 activate 시킨다.\n",
        "# network에게 현재 validatoin한다는 것을 알리기 위해 net.eval() 활성화\n",
        "with torch.no_grad():\n",
        "    net.eval()\n",
        "    loss_mse = []\n",
        "\n",
        "    for batch, data in enumerate(loader_test, 1):\n",
        "        # forward pass\n",
        "        input = data['img'].to(device)\n",
        "\n",
        "        #normalization\n",
        "        input = (input - 0.5) / 0.5\n",
        "\n",
        "        output = net(input)\n",
        "\n",
        "\n",
        "        print(\"TEST: BATCH %04d / %04d\" %\n",
        "              (batch, num_batch_test))\n",
        "        \n",
        "\n",
        "        # Tensorboard 저장하기\n",
        "        input = fn_tonumpy(fn_denorm(input, mean=0.5, std=0.5))\n",
        "        output = fn_tonumpy(fn_denorm(output, mean=0.5, std=0.5))\n",
        "\n",
        "        for j in range(label.shape[0]):\n",
        "            id = batch_size * (batch - 1) + j\n",
        "\n",
        "            input_ = input[j]\n",
        "            output_ = output[j]\n",
        "\n",
        "            # 결과를 png파일로 저장장\n",
        "            input_ = np.clip(input_, a_min=0, a_max=1)\n",
        "            output_ = np.clip(output_, a_min=0, a_max=1)\n",
        "\n",
        "            plt.imsave(os.path.join(result_dir_test, 'png', '%04d_input.png' % id), input_, cmap=cmap)\n",
        "            plt.imsave(os.path.join(result_dir_test, 'png', '%04d_output.png' % id), output_, cmap=cmap)\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TEST: BATCH 0001 / 0010\n",
            "TEST: BATCH 0002 / 0010\n",
            "TEST: BATCH 0003 / 0010\n",
            "TEST: BATCH 0004 / 0010\n",
            "TEST: BATCH 0005 / 0010\n",
            "TEST: BATCH 0006 / 0010\n",
            "TEST: BATCH 0007 / 0010\n",
            "TEST: BATCH 0008 / 0010\n",
            "TEST: BATCH 0009 / 0010\n",
            "TEST: BATCH 0010 / 0010\n",
            "AVERAGE TEST: BATCH 0010 / 0010\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}