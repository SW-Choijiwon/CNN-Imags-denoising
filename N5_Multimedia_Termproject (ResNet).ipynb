{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "N4_Multimedia_Termproject.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BQmYo2rqmK4"
      },
      "source": [
        "# Get Dataset from Google Drive  \n",
        "Please upload your dataset on google drive first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vxHB3wfW911",
        "outputId": "a2438ada-a236-416d-fb17-6ac2c3ff84dc"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbDhYfKKa41s"
      },
      "source": [
        "import os\n",
        "import zipfile\n",
        "import tqdm\n",
        "\n",
        "file_name = \"Multimedia_dataset.zip\"\n",
        "zip_path = os.path.join('/content/drive/MyDrive/Multimedia/Multimedia_dataset/Multimedia_dataset.zip')\n",
        "\n",
        "!cp \"{zip_path}\" .\n",
        "!unzip -q \"{file_name}\"\n",
        "!rm \"{file_name}\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmbp65jgq67j"
      },
      "source": [
        "# Noise Transform  \n",
        "If you want to change how much noise you are giving, change the stddev and mean values at 'gaussian_noise' function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyeUBdtKYQSu"
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torchvision import transforms\n",
        "\n",
        "import random\n",
        "\n",
        "class NoiseTransform(object):\n",
        "  def __init__(self, size=180, mode=\"training\"):\n",
        "    super(NoiseTransform, self).__init__()\n",
        "    self.size = size\n",
        "    self.mode = mode\n",
        "  \n",
        "  def gaussian_noise(self, img):\n",
        "    mean = 0\n",
        "    stddev = 25\n",
        "    noise = Variable(torch.zeros(img.size()))\n",
        "    noise = noise.data.normal_(mean, stddev/255.)\n",
        "\n",
        "    return noise\n",
        "\n",
        "  def __call__(self, img):\n",
        "    if (self.mode == \"training\") | (self.mode == \"validation\"):\n",
        "      self.gt_transform = transforms.Compose([\n",
        "        transforms.Resize((self.size, self.size), interpolation=2),\n",
        "        transforms.ToTensor()])\n",
        "      self.noise_transform = transforms.Compose([\n",
        "        transforms.Resize((self.size, self.size), interpolation=2),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Lambda(self.gaussian_noise),\n",
        "      ])\n",
        "      return self.gt_transform(img), self.noise_transform(img)\n",
        "\n",
        "    elif self.mode == \"testing\":\n",
        "      self.gt_transform = transforms.Compose([\n",
        "        transforms.ToTensor()])\n",
        "      return self.gt_transform(img)\n",
        "    else:\n",
        "      return NotImplementedError\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGKbE7uFrWwb"
      },
      "source": [
        "# Dataloader for Noise Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxZsXXZ9YpYp"
      },
      "source": [
        "import torch\n",
        "import torch.utils.data  as data\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class NoiseDataset(data.Dataset):\n",
        "  def __init__(self, root_path, size):\n",
        "    super(NoiseDataset, self).__init__()\n",
        "\n",
        "    self.root_path = root_path\n",
        "    self.size = size\n",
        "    self.transforms = None\n",
        "    self.examples = None\n",
        "\n",
        "  def set_mode(self, mode):\n",
        "    self.mode = mode\n",
        "    self.transforms = NoiseTransform(self.size, mode)\n",
        "    if mode == \"training\":\n",
        "      train_dir = os.path.join(self.root_path, \"train\")\n",
        "      self.examples = [os.path.join(self.root_path, \"train\", dirs) for dirs in os.listdir(train_dir)]\n",
        "    elif mode == \"validation\":\n",
        "      val_dir = os.path.join(self.root_path, \"validation\")\n",
        "      self.examples = [os.path.join(self.root_path, \"validation\", dirs) for dirs in os.listdir(val_dir)]\n",
        "    elif mode == \"testing\":\n",
        "      test_dir = os.path.join(self.root_path, \"test\")\n",
        "      self.examples = [os.path.join(self.root_path, \"test\", dirs) for dirs in os.listdir(test_dir)]\n",
        "    else:\n",
        "      raise NotImplementedError\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.examples)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    file_name = self.examples[idx]\n",
        "    image = Image.open(file_name)\n",
        "\n",
        "\n",
        "    if self.mode == \"testing\":\n",
        "      input_img = self.transforms(image)\n",
        "      sample = {\"img\": input_img,\n",
        "                \"file_name\": \"image_%06d.png\" % int(os.path.basename(file_name).split('.')[0])}\n",
        "    else:\n",
        "      clean, noise = self.transforms(image)\n",
        "      sample = {\"img\": clean, \"noise\": noise}\n",
        "\n",
        "    return sample"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDx4vFU-rf5z"
      },
      "source": [
        "# Example for Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53tDtBFLenTz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "3fea6616-5e0b-4526-81c4-d037e291a73c"
      },
      "source": [
        "import torch\n",
        "import torch.utils.data  as data\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms\n",
        "import tqdm\n",
        "from PIL import Image\n",
        "\n",
        "def image_show(img):\n",
        "  if isinstance(img, torch.Tensor):\n",
        "    # PIL image로 바꿔준다.\n",
        "    img = transforms.ToPILImage()(img)\n",
        "  plt.imshow(img)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# Change to your data root directory\n",
        "root_path = \"/content/\"\n",
        "# Depend on runtime setting\n",
        "use_cuda = True\n",
        "\n",
        "train_dataset = NoiseDataset(root_path, 128) #128은 size\n",
        "train_dataset.set_mode(\"training\")\n",
        "\n",
        "\n",
        "# batch=4 단위로 data를 load\n",
        "train_dataloader = data.DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "\"\"\"\n",
        "# tqdm은 진행표시바\n",
        "for i, data in enumerate(tqdm.tqdm(train_dataloader)):\n",
        "  #CUDA는 NVIDIA에서 개발한 GPU 개발 툴로 많은 양의 연산을 동시에 처리하는 것이 목표\n",
        "  if use_cuda:\n",
        "    img = data[\"img\"].to('cuda')\n",
        "    noise = data[\"noise\"].to('cuda')\n",
        "  \n",
        "  model_input = img + noise\n",
        "\n",
        "  # clamp는 최대 최소 값을 정해주는 함수\n",
        "  # (최소~최대 범위에 포함되지 않는게 있으면 그 값을 최소 최대값으로 변경)\n",
        "  noise_image = torch.clamp(model_input, 0, 1)\n",
        "\n",
        "  image_show(img[0])\n",
        "  image_show(noise[0])\n",
        "\n",
        "\n",
        "\n",
        "  input()\n",
        "\"\"\""
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n# tqdm은 진행표시바\\nfor i, data in enumerate(tqdm.tqdm(train_dataloader)):\\n  #CUDA는 NVIDIA에서 개발한 GPU 개발 툴로 많은 양의 연산을 동시에 처리하는 것이 목표\\n  if use_cuda:\\n    img = data[\"img\"].to(\\'cuda\\')\\n    noise = data[\"noise\"].to(\\'cuda\\')\\n  \\n  model_input = img + noise\\n\\n  # clamp는 최대 최소 값을 정해주는 함수\\n  # (최소~최대 범위에 포함되지 않는게 있으면 그 값을 최소 최대값으로 변경)\\n  noise_image = torch.clamp(model_input, 0, 1)\\n\\n  image_show(img[0])\\n  image_show(noise[0])\\n\\n\\n\\n  input()\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsbiLHm6CJ9z"
      },
      "source": [
        "# Loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2b7gIV_LCO_6"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MS_SSIM_L1_LOSS(nn.Module):\n",
        "    # Have to use cuda, otherwise the speed is too slow.\n",
        "    def __init__(self, gaussian_sigmas=[0.5, 1.0, 2.0, 4.0, 8.0],\n",
        "                 data_range = 1.0,\n",
        "                 K=(0.01, 0.03),\n",
        "                 alpha=0.025,\n",
        "                 compensation=200.0,\n",
        "                 cuda_dev=0,):\n",
        "        super(MS_SSIM_L1_LOSS, self).__init__()\n",
        "        self.DR = data_range\n",
        "        self.C1 = (K[0] * data_range) ** 2\n",
        "        self.C2 = (K[1] * data_range) ** 2\n",
        "        self.pad = int(2 * gaussian_sigmas[-1])\n",
        "        self.alpha = alpha\n",
        "        self.compensation=compensation\n",
        "        filter_size = int(4 * gaussian_sigmas[-1] + 1)\n",
        "        g_masks = torch.zeros((3*len(gaussian_sigmas), 1, filter_size, filter_size))\n",
        "        for idx, sigma in enumerate(gaussian_sigmas):\n",
        "            # r0,g0,b0,r1,g1,b1,...,rM,gM,bM\n",
        "            g_masks[3*idx+0, 0, :, :] = self._fspecial_gauss_2d(filter_size, sigma)\n",
        "            g_masks[3*idx+1, 0, :, :] = self._fspecial_gauss_2d(filter_size, sigma)\n",
        "            g_masks[3*idx+2, 0, :, :] = self._fspecial_gauss_2d(filter_size, sigma)\n",
        "        self.g_masks = g_masks.cuda(cuda_dev)\n",
        "\n",
        "    def _fspecial_gauss_1d(self, size, sigma):\n",
        "        \"\"\"Create 1-D gauss kernel\n",
        "        Args:\n",
        "            size (int): the size of gauss kernel\n",
        "            sigma (float): sigma of normal distribution\n",
        "        Returns:\n",
        "            torch.Tensor: 1D kernel (size)\n",
        "        \"\"\"\n",
        "        coords = torch.arange(size).to(dtype=torch.float)\n",
        "        coords -= size // 2\n",
        "        g = torch.exp(-(coords ** 2) / (2 * sigma ** 2))\n",
        "        g /= g.sum()\n",
        "        return g.reshape(-1)\n",
        "\n",
        "    def _fspecial_gauss_2d(self, size, sigma):\n",
        "        \"\"\"Create 2-D gauss kernel\n",
        "        Args:\n",
        "            size (int): the size of gauss kernel\n",
        "            sigma (float): sigma of normal distribution\n",
        "        Returns:\n",
        "            torch.Tensor: 2D kernel (size x size)\n",
        "        \"\"\"\n",
        "        gaussian_vec = self._fspecial_gauss_1d(size, sigma)\n",
        "        return torch.outer(gaussian_vec, gaussian_vec)\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        b, c, h, w = x.shape\n",
        "        mux = F.conv2d(x, self.g_masks, groups=3, padding=self.pad)\n",
        "        muy = F.conv2d(y, self.g_masks, groups=3, padding=self.pad)\n",
        "\n",
        "        mux2 = mux * mux\n",
        "        muy2 = muy * muy\n",
        "        muxy = mux * muy\n",
        "\n",
        "        sigmax2 = F.conv2d(x * x, self.g_masks, groups=3, padding=self.pad) - mux2\n",
        "        sigmay2 = F.conv2d(y * y, self.g_masks, groups=3, padding=self.pad) - muy2\n",
        "        sigmaxy = F.conv2d(x * y, self.g_masks, groups=3, padding=self.pad) - muxy\n",
        "\n",
        "        # l(j), cs(j) in MS-SSIM\n",
        "        l  = (2 * muxy    + self.C1) / (mux2    + muy2    + self.C1)  # [B, 15, H, W]\n",
        "        cs = (2 * sigmaxy + self.C2) / (sigmax2 + sigmay2 + self.C2)\n",
        "\n",
        "        lM = l[:, -1, :, :] * l[:, -2, :, :] * l[:, -3, :, :]\n",
        "        PIcs = cs.prod(dim=1)\n",
        "\n",
        "        loss_ms_ssim = 1 - lM*PIcs  # [B, H, W]\n",
        "\n",
        "        loss_l1 = F.l1_loss(x, y, reduction='none')  # [B, 3, H, W]\n",
        "        # average l1 loss in 3 channels\n",
        "        gaussian_l1 = F.conv2d(loss_l1, self.g_masks.narrow(dim=0, start=-3, length=3),\n",
        "                               groups=3, padding=self.pad).mean(1)  # [B, H, W]\n",
        "\n",
        "        loss_mix = self.alpha * loss_ms_ssim + (1 - self.alpha) * gaussian_l1 / self.DR\n",
        "        loss_mix = self.compensation*loss_mix\n",
        "\n",
        "        return loss_mix.mean()\n",
        "\n",
        "# https://github.com/psyrocloud/MS-SSIM_L1_LOSS"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtESD4NV4Wks"
      },
      "source": [
        "# UNet Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVJI37g45PLz"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "## 네트워크 구축하기\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, nch, nker=64, learning_type=\"plain\", norm=\"bnorm\"):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        self.learning_type = learning_type\n",
        "\n",
        "        # Contracting path\n",
        "        self.enc1_1 = CBR2d(in_channels=nch, out_channels=1 * nker, norm=norm)\n",
        "        self.enc1_2 = CBR2d(in_channels=1 * nker, out_channels=1 * nker, norm=norm)\n",
        "\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.enc2_1 = CBR2d(in_channels=nker, out_channels=2 * nker, norm=norm)\n",
        "        self.enc2_2 = CBR2d(in_channels=2 * nker, out_channels=2 * nker, norm=norm)\n",
        "\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.enc3_1 = CBR2d(in_channels=2 * nker, out_channels=4 * nker, norm=norm)\n",
        "        self.enc3_2 = CBR2d(in_channels=4 * nker, out_channels=4 * nker, norm=norm)\n",
        "\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.enc4_1 = CBR2d(in_channels=4 * nker, out_channels=8 * nker, norm=norm)\n",
        "        self.enc4_2 = CBR2d(in_channels=8 * nker, out_channels=8 * nker, norm=norm)\n",
        "\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.enc5_1 = CBR2d(in_channels=8 * nker, out_channels=16 * nker, norm=norm)\n",
        "\n",
        "\n",
        "        # Expansive path\n",
        "        self.dec5_1 = CBR2d(in_channels=16 * nker, out_channels=8 * nker, norm=norm)\n",
        "\n",
        "        self.unpool4 = nn.ConvTranspose2d(in_channels=8 * nker, out_channels=8 * nker,\n",
        "                                          kernel_size=2, stride=2, padding=0, bias=True)\n",
        "\n",
        "        self.dec4_2 = CBR2d(in_channels=2 * 8 * nker, out_channels=8 * nker, norm=norm)\n",
        "        self.dec4_1 = CBR2d(in_channels=8 * nker, out_channels=4 * nker, norm=norm)\n",
        "\n",
        "        self.unpool3 = nn.ConvTranspose2d(in_channels=4 * nker, out_channels=4 * nker,\n",
        "                                          kernel_size=2, stride=2, padding=0, bias=True)\n",
        "\n",
        "        self.dec3_2 = CBR2d(in_channels=2 * 4 * nker, out_channels=4 * nker, norm=norm)\n",
        "        self.dec3_1 = CBR2d(in_channels=4 * nker, out_channels=2 * nker, norm=norm)\n",
        "\n",
        "        self.unpool2 = nn.ConvTranspose2d(in_channels=2 * nker, out_channels=2 * nker,\n",
        "                                          kernel_size=2, stride=2, padding=0, bias=True)\n",
        "\n",
        "        self.dec2_2 = CBR2d(in_channels=2 * 2 * nker, out_channels=2 * nker, norm=norm)\n",
        "        self.dec2_1 = CBR2d(in_channels=2 * nker, out_channels=1 * nker, norm=norm)\n",
        "\n",
        "        self.unpool1 = nn.ConvTranspose2d(in_channels=1 * nker, out_channels=1 * nker,\n",
        "                                          kernel_size=2, stride=2, padding=0, bias=True)\n",
        "\n",
        "        self.dec1_2 = CBR2d(in_channels=2 * 1 * nker, out_channels=1 * nker, norm=norm)\n",
        "        self.dec1_1 = CBR2d(in_channels=1 * nker, out_channels=1 * nker, norm=norm)\n",
        "\n",
        "        self.fc = nn.Conv2d(in_channels=1 * nker, out_channels=nch, kernel_size=1, stride=1, padding=0, bias=True)\n",
        "\n",
        "    # Unet Layer 연결 (Forwarding)\n",
        "    # 위에서 정의한 것을 순서대로 실행한다고 생각하면 됨\n",
        "    def forward(self, x):\n",
        "        # forward encoder\n",
        "        enc1_1 = self.enc1_1(x)\n",
        "        enc1_2 = self.enc1_2(enc1_1)\n",
        "        pool1 = self.pool1(enc1_2)\n",
        "\n",
        "        enc2_1 = self.enc2_1(pool1)\n",
        "        enc2_2 = self.enc2_2(enc2_1)\n",
        "        pool2 = self.pool2(enc2_2)\n",
        "\n",
        "        enc3_1 = self.enc3_1(pool2)\n",
        "        enc3_2 = self.enc3_2(enc3_1)\n",
        "        pool3 = self.pool3(enc3_2)\n",
        "\n",
        "        enc4_1 = self.enc4_1(pool3)\n",
        "        enc4_2 = self.enc4_2(enc4_1)\n",
        "        pool4 = self.pool4(enc4_2)\n",
        "\n",
        "        enc5_1 = self.enc5_1(pool4)\n",
        "\n",
        "        # forward decoder\n",
        "        dec5_1 = self.dec5_1(enc5_1)\n",
        "\n",
        "        unpool4 = self.unpool4(dec5_1)\n",
        "        # cat은 이전 step의 output channel과 skip connection을 연결해주는 부분\n",
        "        # dim = [0: batch, 1: channel, 2: height, 3: width] <- dim은 해당 방향을 알려준다.\n",
        "        cat4 = torch.cat((unpool4, enc4_2), dim=1)\n",
        "        dec4_2 = self.dec4_2(cat4)\n",
        "        dec4_1 = self.dec4_1(dec4_2)\n",
        "\n",
        "        unpool3 = self.unpool3(dec4_1)\n",
        "        cat3 = torch.cat((unpool3, enc3_2), dim=1)\n",
        "        dec3_2 = self.dec3_2(cat3)\n",
        "        dec3_1 = self.dec3_1(dec3_2)\n",
        "\n",
        "        unpool2 = self.unpool2(dec3_1)\n",
        "        cat2 = torch.cat((unpool2, enc2_2), dim=1)\n",
        "        dec2_2 = self.dec2_2(cat2)\n",
        "        dec2_1 = self.dec2_1(dec2_2)\n",
        "\n",
        "        unpool1 = self.unpool1(dec2_1)\n",
        "        cat1 = torch.cat((unpool1, enc1_2), dim=1)\n",
        "        dec1_2 = self.dec1_2(cat1)\n",
        "        dec1_1 = self.dec1_1(dec1_2)\n",
        "\n",
        "        if self.learning_type == \"plain\":\n",
        "            x = self.fc(dec1_1)\n",
        "        elif self.learning_type == \"residual\":\n",
        "            x = x + self.fc(dec1_1)\n",
        "\n",
        "        return x\n",
        "  \n",
        "## 네트워크 저장하기\n",
        "def save(ckpt_dir, net, optim, epoch):\n",
        "    if not os.path.exists(ckpt_dir):\n",
        "        os.makedirs(ckpt_dir)\n",
        "\n",
        "    torch.save({'net': net.state_dict(), 'optim': optim.state_dict()},\n",
        "               \"%s/model_epoch%d.pth\" % (ckpt_dir, epoch))\n",
        "\n",
        "## 네트워크 불러오기\n",
        "def load(ckpt_dir, net, optim):\n",
        "    if not os.path.exists(ckpt_dir):\n",
        "        epoch = 0\n",
        "        return net, optim, epoch\n",
        "\n",
        "    ckpt_lst = os.listdir(ckpt_dir)\n",
        "    ckpt_lst.sort(key=lambda f: int(''.join(filter(str.isdigit, f))))\n",
        "\n",
        "    dict_model = torch.load('%s/%s' % (ckpt_dir, ckpt_lst[-1]))\n",
        "\n",
        "    net.load_state_dict(dict_model['net'])\n",
        "    optim.load_state_dict(dict_model['optim'])\n",
        "    epoch = int(ckpt_lst[-1].split('epoch')[1].split('.pth')[0])\n",
        "\n",
        "    return net, optim, epoch\n",
        "\n",
        "# convolution, batch normalizatoin, Relu\n",
        "class CBR2d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True, norm=\"bnorm\", relu=0.0):\n",
        "        super().__init__()\n",
        "\n",
        "        layers = []\n",
        "        layers += [nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
        "                             kernel_size=kernel_size, stride=stride, padding=padding,\n",
        "                             bias=bias)]\n",
        "\n",
        "        if not norm is None:\n",
        "            if norm == \"bnorm\":\n",
        "                layers += [nn.BatchNorm2d(num_features=out_channels)]\n",
        "            elif norm == \"inorm\":\n",
        "                layers += [nn.InstanceNorm2d(num_features=out_channels)]\n",
        "\n",
        "        if not relu is None and relu >= 0.0:\n",
        "            layers += [nn.ReLU() if relu == 0 else nn.LeakyReLU(relu)]\n",
        "\n",
        "        self.cbr = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.cbr(x)\n",
        "\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93Dg5Dd8O3sj"
      },
      "source": [
        "# ResNet Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9yYhiKjO7fE"
      },
      "source": [
        "class ResNet(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, nker=64,\n",
        "                 learning_type=\"plain\", norm=\"bnorm\", nblk=16):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.learning_type = learning_type\n",
        "\n",
        "        self.enc = CBR2d(in_channels=in_channels, out_channels= nker,\n",
        "                         kernel_size=3, stride=1, padding=1,\n",
        "                         bias=True, norm=None, relu=0.0)\n",
        "\n",
        "        # res block정의\n",
        "        res = []\n",
        "        for i in range(nblk):\n",
        "            res += [ResBlock(nker, nker, kernel_size=3, stride=1,\n",
        "                             padding=1, bias=True, norm=norm, relu=0.0)]\n",
        "        self.res = nn.Sequential(*res)\n",
        "\n",
        "        # encoder part\n",
        "        self.dec = CBR2d(nker, nker, kernel_size=3, stride=1,\n",
        "                             padding=1, bias=True, norm=norm, relu=0.0)\n",
        "\n",
        "        # single convolution layer 생성\n",
        "        self.fc = nn.Conv2d(in_channels=nker, out_channels=out_channels,\n",
        "                            kernel_size=1, stride=1, padding=0, bias=True)\n",
        "\n",
        "    # forward function\n",
        "    def forward(self, x):\n",
        "        x0 =x\n",
        "\n",
        "        x = self.enc(x)\n",
        "        x = self.res(x)\n",
        "        x = self.dec(x)\n",
        "\n",
        "        if self.learning_type == \"plain\":\n",
        "            x = self.fc(x)\n",
        "        elif self.learning_type == \"residual\":\n",
        "            x = x0 + self.fc(x)\n",
        "\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels,\n",
        "                 kernel_size=3, stride=1, padding=1,\n",
        "                 bias=True, norm=\"bnorm\", relu=0.0):\n",
        "        super().__init__()\n",
        "\n",
        "        layers =[]\n",
        "\n",
        "        # 1st CBR2d\n",
        "        layers += [CBR2d(in_channels, out_channels,\n",
        "                        kernel_size=kernel_size, stride=stride,\n",
        "                        padding=padding, bias=bias, norm=norm, relu=relu)]\n",
        "\n",
        "        # 2nd CBR2d\n",
        "        layers += [CBR2d(in_channels, out_channels,\n",
        "                        kernel_size=kernel_size, stride=stride,\n",
        "                        padding=padding, bias=bias, norm=norm, relu=None)]\n",
        "\n",
        "        self.resblk = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.resblk(x)\n",
        "\n",
        "\n",
        "## 네트워크 저장하기\n",
        "def save(ckpt_dir, net, optim, epoch):\n",
        "    if not os.path.exists(ckpt_dir):\n",
        "        os.makedirs(ckpt_dir)\n",
        "\n",
        "    torch.save({'net': net.state_dict(), 'optim': optim.state_dict()},\n",
        "               \"%s/model_epoch%d.pth\" % (ckpt_dir, epoch))\n",
        "\n",
        "## 네트워크 불러오기\n",
        "def load(ckpt_dir, net, optim):\n",
        "    if not os.path.exists(ckpt_dir):\n",
        "        epoch = 0\n",
        "        return net, optim, epoch\n",
        "\n",
        "    ckpt_lst = os.listdir(ckpt_dir)\n",
        "    ckpt_lst.sort(key=lambda f: int(''.join(filter(str.isdigit, f))))\n",
        "\n",
        "    dict_model = torch.load('%s/%s' % (ckpt_dir, ckpt_lst[-1]))\n",
        "\n",
        "    net.load_state_dict(dict_model['net'])\n",
        "    optim.load_state_dict(dict_model['optim'])\n",
        "    epoch = int(ckpt_lst[-1].split('epoch')[1].split('.pth')[0])\n",
        "\n",
        "    return net, optim, epoch\n",
        "\n",
        "# convolution, batch normalizatoin, Relu\n",
        "class CBR2d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True, norm=\"bnorm\", relu=0.0):\n",
        "        super().__init__()\n",
        "\n",
        "        layers = []\n",
        "        layers += [nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
        "                             kernel_size=kernel_size, stride=stride, padding=padding,\n",
        "                             bias=bias)]\n",
        "\n",
        "        if not norm is None:\n",
        "            if norm == \"bnorm\":\n",
        "                layers += [nn.BatchNorm2d(num_features=out_channels)]\n",
        "            elif norm == \"inorm\":\n",
        "                layers += [nn.InstanceNorm2d(num_features=out_channels)]\n",
        "\n",
        "        if not relu is None and relu >= 0.0:\n",
        "            layers += [nn.ReLU() if relu == 0 else nn.LeakyReLU(relu)]\n",
        "\n",
        "        self.cbr = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.cbr(x)\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xh1CUC654Q87"
      },
      "source": [
        "# Trainer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcjIWFPo4VeW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 664
        },
        "outputId": "2453cdb8-103e-4485-9539-1565754d367a"
      },
      "source": [
        "import argparse\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "#import pytorch_ssim\n",
        "!pip install pytorch_msssim \n",
        "import pytorch_msssim\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "## 트레이닝 파라메터 설정하기\n",
        "train_continue = \"off\"\n",
        "\n",
        "lr = 1e-4 #learning rate\n",
        "batch_size = 30\n",
        "num_epoch = 100\n",
        "\n",
        "ckpt_dir = \"/content/drive/MyDrive/Multimedia/Termproject/ResNet/checkpoint/\"\n",
        "log_dir = \"/content/drive/MyDrive/Multimedia/Termproject/ResNet/log\"\n",
        "result_dir = \"/content/drive/MyDrive/Multimedia/Termproject/ResNet/result\"\n",
        "\n",
        "opts = ['random', 4]\n",
        "\n",
        "\n",
        "nch = 3\n",
        "nker = 64\n",
        "\n",
        "learning_type = 'plain'\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "\n",
        "print(\"learning rate: %.4e\" % lr)\n",
        "print(\"batch size: %d\" % batch_size)\n",
        "print(\"number of epoch: %d\" % num_epoch)\n",
        "\n",
        "print(\"opts: %s\" % opts)\n",
        "\n",
        "print(\"learning type: %s\" % learning_type)\n",
        "\n",
        "print(\"ckpt dir: %s\" % ckpt_dir)\n",
        "print(\"log dir: %s\" % log_dir)\n",
        "print(\"result dir: %s\" % result_dir)\n",
        "\n",
        "print(\"device: %s\" % device)\n",
        "\n",
        "## 디렉토리 생성하기\n",
        "result_dir_train = os.path.join(result_dir, 'train')\n",
        "result_dir_val = os.path.join(result_dir, 'val')\n",
        "\n",
        "if not os.path.exists(result_dir):\n",
        "    os.makedirs(os.path.join(result_dir_train, 'png'))\n",
        "    # os.makedirs(os.path.join(result_dir_train, 'numpy'))\n",
        "\n",
        "    os.makedirs(os.path.join(result_dir_val, 'png'))\n",
        "    # os.makedirs(os.path.join(result_dir_val, 'numpy'))\n",
        "\n",
        "    os.makedirs(os.path.join(result_dir_test, 'png'))\n",
        "    os.makedirs(os.path.join(result_dir_test, 'numpy'))\n",
        "\n",
        "\n",
        "# Change to your data root directory\n",
        "root_path = \"/content/\"\n",
        "# Depend on runtime setting\n",
        "use_cuda = True\n",
        "\n",
        "\n",
        "\n",
        "# training dataset 불러오기\n",
        "dataset_train = NoiseDataset(root_path, 128) #128은 size\n",
        "dataset_train.set_mode(\"training\")\n",
        "loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "\n",
        "# validation dataset불러오기\n",
        "dataset_val = NoiseDataset(root_path, 128) #128은 size\n",
        "dataset_val.set_mode(\"validation\")\n",
        "loader_val = DataLoader(dataset_val, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "\n",
        "# 그밖에 부수적인 variables 설정하기\n",
        "num_data_train = len(dataset_train)\n",
        "num_data_val = len(dataset_val)\n",
        "\n",
        "num_batch_train = np.ceil(num_data_train / batch_size)\n",
        "num_batch_val = np.ceil(num_data_val / batch_size)\n",
        "\n",
        "\n",
        "\n",
        "## 네트워크 생성하기\n",
        "#net = UNet(nch=nch, nker=nker, learning_type=learning_type).to(device)\n",
        "net = ResNet(in_channels= nch, out_channels = nch, nker=nker, learning_type=learning_type, nblk=16).to(device)\n",
        "\n",
        "## 손실함수 정의하기\n",
        "#fn_loss = nn.MSELoss().to(device)\n",
        "fn_loss = nn.L1Loss().to(device)\n",
        "#fn_loss = pytorch_msssim.SSIM().to(device)\n",
        "#fn_loss = MS_SSIM_L1_LOSS()\n",
        "mse_loss = nn.MSELoss().to(device)\n",
        "\n",
        "## Optimizer 설정하기\n",
        "# adam optimizer사용\n",
        "optim = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "\n",
        "#scheduler\n",
        "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer=optim,\n",
        "                                        lr_lambda=lambda epoch: 0.95 ** epoch,\n",
        "                                        last_epoch=-1,\n",
        "                                        verbose=False)\n",
        "\n",
        "\n",
        "\n",
        "## 그밖에 부수적인 functions 설정하기\n",
        "# output을 저장하기 위해 필요한 몇가지 함수들\n",
        "\n",
        "\n",
        "# tensor에서 numpy로 변환하는 함수\n",
        "fn_tonumpy = lambda x: x.to('cpu').detach().numpy().transpose(0, 2, 3, 1)\n",
        "# normalize된 data를 반대로 denomalization시키는 함수\n",
        "fn_denorm = lambda x, mean, std: (x * std) + mean\n",
        "# 네트워크 output의 이미지를 binary class로 분류해주는 함수\n",
        "fn_class = lambda x: 1.0 * (x > 0.5)\n",
        "\n",
        "cmap = None\n",
        "\n",
        "\n",
        "## Tensorboard 를 사용하기 위한 SummaryWriter 설정\n",
        "writer_train = SummaryWriter(log_dir=os.path.join(log_dir, 'train'))\n",
        "writer_val: SummaryWriter = SummaryWriter(log_dir=os.path.join(log_dir, 'val'))\n",
        "\n",
        "\n",
        "## 네트워크 학습시키기\n",
        "# training이 시작되는 epoch의 시작점 0으로 세팅\n",
        "st_epoch = 0\n",
        "\n",
        "\n",
        "# TRAIN \n",
        "if train_continue == \"on\":\n",
        "    net, optim, st_epoch = load(ckpt_dir=ckpt_dir, net=net, optim=optim)\n",
        "    #사전에 저장이 된 network가 있다면 연속적으로 학습하기 위해 불러와서 사용\n",
        "\n",
        "\n",
        "# training을 한다고 network에 알림림\n",
        "for epoch in range(st_epoch + 1, num_epoch + 1):\n",
        "    net.train()\n",
        "    loss_list = []\n",
        "    loss_mse_list = []\n",
        "\n",
        "\n",
        "    # network이 input을 받아 output을 출력하는 forward pass\n",
        "    for batch, data in enumerate(loader_train, 1):\n",
        "        # forward pass\n",
        "        label = data['img'].to(device)\n",
        "        noise = data['noise'].to(device)\n",
        "\n",
        "        model_input = label + noise\n",
        "        input = torch.clamp(model_input, 0, 1)\n",
        "\n",
        "        #normalization\n",
        "        input = (input - 0.5) / 0.5\n",
        "        label = (label - 0.5) / 0.5\n",
        "\n",
        "        output = net(input)\n",
        "\n",
        "\n",
        "        # backpropagation을 한느 부분\n",
        "        # backward pass\n",
        "        optim.zero_grad()\n",
        "\n",
        "        #print(output.shape)\n",
        "        #print(label.shape)\n",
        "\n",
        "        loss = fn_loss(output, label)\n",
        "        loss_list +=[loss.item()]\n",
        "        loss.backward()\n",
        "\n",
        "        optim.step()\n",
        "        scheduler.step() \n",
        "\n",
        "        \n",
        "        # mse loss 계산\n",
        "        loss_mse = mse_loss(output, label)\n",
        "        loss_mse_list += [loss_mse.item()]\n",
        "\n",
        "        print(\"TRAIN: EPOCH %04d / %04d | BATCH %04d / %04d | L1+MS-SSIM LOSS %.4f | MSE LOSS %.4f | ACCURACY %.4f\" %\n",
        "                  (epoch, num_epoch, batch, num_batch_train, np.mean(loss_list), np.mean(loss_mse_list), 1-np.mean(loss_mse_list)))\n",
        "\n",
        "        if batch % 10 == 0:\n",
        "          # Tensorboard 저장하기\n",
        "          # Tensorboard에 input, output, label을 저장\n",
        "          label = fn_tonumpy(fn_denorm(label, mean=0.5, std=0.5))\n",
        "          input = fn_tonumpy(fn_denorm(input, mean=0.5, std=0.5))\n",
        "          output = fn_tonumpy(fn_denorm(output, mean=0.5, std=0.5))\n",
        "\n",
        "          input = np.clip(input, a_min=0, a_max=1)\n",
        "          output = np.clip(output, a_min=0, a_max=1)\n",
        "\n",
        "          id = num_batch_train * (epoch - 1) + batch\n",
        "\n",
        "          plt.imsave(os.path.join(result_dir_train, 'png', '%04d_label.png' % id), label[0].squeeze(), cmap=cmap)\n",
        "          plt.imsave(os.path.join(result_dir_train, 'png', '%04d_input.png' % id), input[0].squeeze(), cmap=cmap)\n",
        "          plt.imsave(os.path.join(result_dir_train, 'png', '%04d_output.png' % id), output[0].squeeze(), cmap=cmap)\n",
        "\n",
        "\n",
        "    # loss 를 tensorboard에 저장\n",
        "    writer_train.add_scalar('loss', np.mean(loss_mse_list), epoch)\n",
        "    \n",
        "#=============================================================================네트워크를 training하는 부분 끝\n",
        "    # network validation하는 부분\n",
        "    # validatoin부분은 backpropagation이 없기에 이를 사전에 막기 위해 torch.no_grad()를 activate 시킨다.\n",
        "    # network에게 현재 validatoin한다는 것을 알리기 위해 net.eval() 활성화\n",
        "    with torch.no_grad():\n",
        "        net.eval()\n",
        "        loss_list = []\n",
        "        loss_mse_list = []\n",
        "\n",
        "        # training과 마찬가지로 forward pass진행\n",
        "        for batch, data in enumerate(loader_val, 1):\n",
        "            # forward pass\n",
        "            label = data['img'].to(device)\n",
        "            noise = data['noise'].to(device)\n",
        "\n",
        "            model_input = label + noise\n",
        "            input = torch.clamp(model_input, 0, 1)\n",
        "\n",
        "            #normalization\n",
        "            input = (input - 0.5) / 0.5\n",
        "            label = (label - 0.5) / 0.5\n",
        "\n",
        "\n",
        "            output = net(input)\n",
        "\n",
        "\n",
        "            # 손실함수 계산하기\n",
        "            loss = fn_loss(output, label)\n",
        "\n",
        "            # 손실함수 계산\n",
        "            loss_list += [loss.item()]\n",
        "\n",
        "            # mse loss 계산\n",
        "            loss_mse = mse_loss(output, label)\n",
        "            loss_mse_list += [loss_mse.item()]\n",
        "            #loss_psnr = psnr(np.mean(loss_mse))\n",
        "            print(\"VALID: EPOCH %04d / %04d | BATCH %04d / %04d | L1+MS-SSIM LOSS %.4f | MSE LOSS %.4f | ACCURACY %.4f\" %\n",
        "                  (epoch, num_epoch, batch, num_batch_train, np.mean(loss_list), np.mean(loss_mse_list), 1-np.mean(loss_mse_list)))\n",
        "            \n",
        "            if batch % 3 == 0:\n",
        "              label = fn_tonumpy(fn_denorm(label, mean=0.5, std=0.5))\n",
        "              input = fn_tonumpy(fn_denorm(input, mean=0.5, std=0.5))\n",
        "              output = fn_tonumpy(fn_denorm(output, mean=0.5, std=0.5))\n",
        "\n",
        "              input = np.clip(input, a_min=0, a_max=1)\n",
        "              output = np.clip(output, a_min=0, a_max=1)\n",
        "\n",
        "              id = num_batch_val * (epoch - 1) + batch\n",
        "\n",
        "              #결과를 png파일로 저장\n",
        "              plt.imsave(os.path.join(result_dir_val, 'png', '%04d_label.png' % id), label[0].squeeze(), cmap=cmap)\n",
        "              plt.imsave(os.path.join(result_dir_val, 'png', '%04d_input.png' % id), input[0].squeeze(), cmap=cmap)\n",
        "              plt.imsave(os.path.join(result_dir_val, 'png', '%04d_output.png' % id), output[0].squeeze(), cmap=cmap)\n",
        "\n",
        "\n",
        "    writer_val.add_scalar('loss', np.mean(loss_mse_list), epoch)\n",
        "\n",
        "    # 20번마다 한번씩 network저장\n",
        "    if epoch % 20 == 0:\n",
        "      # epoch이 진행 될 때마다 network저장\n",
        "        save(ckpt_dir=ckpt_dir, net=net, optim=optim, epoch=epoch)\n",
        "\n",
        "writer_train.close()\n",
        "writer_val.close()\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch_msssim in /usr/local/lib/python3.7/dist-packages (0.2.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from pytorch_msssim) (1.8.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch->pytorch_msssim) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->pytorch_msssim) (3.7.4.3)\n",
            "learning rate: 1.0000e-04\n",
            "batch size: 30\n",
            "number of epoch: 100\n",
            "opts: ['random', 4]\n",
            "learning type: plain\n",
            "ckpt dir: /content/drive/MyDrive/Multimedia/Termproject/ResNet/checkpoint/\n",
            "log dir: /content/drive/MyDrive/Multimedia/Termproject/ResNet/log\n",
            "result dir: /content/drive/MyDrive/Multimedia/Termproject/ResNet/result\n",
            "device: cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-aaac16926a0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-898ebbf09281>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mx0\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-898ebbf09281>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcbr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1204\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1206\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1207\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 120.00 MiB (GPU 0; 14.76 GiB total capacity; 13.63 GiB already allocated; 63.75 MiB free; 13.66 GiB reserved in total by PyTorch)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-k_5CSnI9Ip"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOzB_NQqI_eN"
      },
      "source": [
        "import argparse\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torchvision import transforms\n",
        "\n",
        "ckpt_dir = \"/content/drive/MyDrive/Multimedia/Test1/checkpoint/\"\n",
        "result_dir = \"/content/drive/MyDrive/Multimedia/Test1/result\"\n",
        "\n",
        "## 디렉토리 생성하기\n",
        "result_dir_test = os.path.join(result_dir, 'test')\n",
        "if not os.path.exists(result_dir):\n",
        "    os.makedirs(os.path.join(result_dir_test, 'png'))\n",
        "    os.makedirs(os.path.join(result_dir_test, 'numpy'))\n",
        "\n",
        "\n",
        "dataset_test = NoiseDataset(root_path, 128) #128은 size\n",
        "dataset_test.set_mode(\"testing\")\n",
        "loader_test = DataLoader(dataset_test, batch_size=1, shuffle=False)\n",
        "\n",
        "\n",
        "# 그밖에 부수적인 variables 설정하기\n",
        "num_data_test = len(dataset_test)\n",
        "num_batch_test = np.ceil(num_data_test / batch_size)\n",
        "\n",
        "\n",
        "## 손실함수 정의하기\n",
        "# fn_loss = nn.BCEWithLogitsLoss().to(device)\n",
        "fn_loss = nn.MSELoss().to(device)\n",
        "\n",
        "\n",
        "## 네트워크 생성하기\n",
        "net = UNet(nch=nch, nker=nker, learning_type=learning_type).to(device)\n",
        "\n",
        "\n",
        "## Optimizer 설정하기\n",
        "# adam optimizer사용\n",
        "optim = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "\n",
        "\n",
        "## 그밖에 부수적인 functions 설정하기\n",
        "# output을 저장하기 위해 필요한 몇가지 함수들\n",
        "\n",
        "\n",
        "# tensor에서 numpy로 변환하는 함수\n",
        "fn_tonumpy = lambda x: x.to('cpu').detach().numpy().transpose(0, 2, 3, 1)\n",
        "# normalize된 data를 반대로 denomalization시키는 함수\n",
        "fn_denorm = lambda x, mean, std: (x * std) + mean\n",
        "# 네트워크 output의 이미지를 binary class로 분류해주는 함수\n",
        "fn_class = lambda x: 1.0 * (x > 0.5)\n",
        "\n",
        "cmap = None\n",
        "\n",
        "net, optim, st_epoch = load(ckpt_dir=ckpt_dir, net=net, optim=optim)\n",
        "\n",
        "\n",
        "# backpropagation이 없기에 이를 사전에 막기 위해 torch.no_grad()를 activate 시킨다.\n",
        "# network에게 현재 validatoin한다는 것을 알리기 위해 net.eval() 활성화\n",
        "with torch.no_grad():\n",
        "    net.eval()\n",
        "    loss_mse = []\n",
        "\n",
        "    for batch, data in enumerate(loader_test, 1):\n",
        "        # forward pass\n",
        "        input = data['img'].to(device)\n",
        "        file_name = data[\"file_name\"]\n",
        "\n",
        "        #normalization\n",
        "        input = (input - 0.5) / 0.5\n",
        "\n",
        "        output = net(input)\n",
        "\n",
        "\n",
        "        print(\"TEST: BATCH %04d / %04d\" %\n",
        "              (batch, num_batch_test))\n",
        "        \n",
        "\n",
        "        # Tensorboard 저장하기\n",
        "        input = fn_tonumpy(fn_denorm(input, mean=0.5, std=0.5))\n",
        "        output = fn_tonumpy(fn_denorm(output, mean=0.5, std=0.5))\n",
        "\n",
        "        for j in range(input.shape[0]):\n",
        "            id = batch_size * (batch - 1) + j\n",
        "\n",
        "            output_ = output[j]\n",
        "\n",
        "            # 결과를 png파일로 저장장\n",
        "            output_ = np.clip(output_, a_min=0, a_max=1)\n",
        "\n",
        "            plt.imsave(os.path.join(result_dir_test, file_name[j]), output_, cmap=cmap)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xu4leXBf5ONe"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data  as data\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import tqdm.notebook as tq\n",
        "from PIL import Image\n",
        "from skimage.measure.simple_metrics import compare_psnr\n",
        "\n",
        "def image_save(img, path):\n",
        "  if isinstance(img, torch.Tensor):\n",
        "    img = transforms.ToPILImage()(img)\n",
        "  img.save(path)\n",
        "\n",
        "def batch_PSNR(img, imclean, data_range):\n",
        "    Img = img.data.cpu().numpy().astype(np.float32)\n",
        "    Iclean = imclean.data.cpu().numpy().astype(np.float32)\n",
        "    PSNR = 0\n",
        "    for i in range(Img.shape[0]):\n",
        "        PSNR += compare_psnr(Iclean[i, :, :, :], Img[i, :, :, :], data_range=data_range)\n",
        "    return (PSNR/Img.shape[0])\n",
        "\n",
        "# Change to your data root directory\n",
        "checkpoint_path = \"/content/drive/MyDrive/Multimedia/Termproject/checkpoint/\"\n",
        "result_save_path = \"/content/drive/MyDrive/Multimedia/Test1/result\"\n",
        "\n",
        "# Depend on runtime setting\n",
        "use_cuda = True\n",
        "\n",
        "test_dataset = NoiseDataset(image_path, 128)\n",
        "test_dataset.set_mode(\"testing\")\n",
        "\n",
        "test_dataloader = data.DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "net = DNCNN()\n",
        "\n",
        "if use_cuda:\n",
        "  net.to('cuda')\n",
        "\n",
        "net.load_state_dict(torch.load(checkpoint_path))\n",
        "model = nn.DataParallel(net)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "for i, data in enumerate(tq.tqdm(test_dataloader)):\n",
        "  if use_cuda:\n",
        "    img = data[\"img\"].to('cuda')\n",
        "  file_name = data[\"file_name\"]\n",
        "\n",
        "  with torch.no_grad():\n",
        "    out_test = torch.clamp(img - model(img), 0., 1.)\n",
        "    for idx in range(len(img)):\n",
        "      image_save(out_test[idx], os.path.join(result_save_path, file_name[idx]))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}